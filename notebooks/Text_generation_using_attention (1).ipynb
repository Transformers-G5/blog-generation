{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj0e8obvNSmO",
        "outputId": "850f5658-b92b-497e-83ed-5318669ad631"
      },
      "outputs": [],
      "source": [
        "# !pip install \"tensorflow-text\"\n",
        "# !pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Ep2oKhNaH0"
      },
      "outputs": [],
      "source": [
        "#EXP\n",
        "# import pandas as pd\n",
        "# convert csv to txt\n",
        "def csv_to_txt():\n",
        "  import pandas as pd\n",
        "  path = '/content/captions_csv.csv'\n",
        "  data = pd.read_csv(path)\n",
        "  data.dropna()\n",
        "  print(data.head())\n",
        "  column_contents = data['Caption'].astype(str).values.tolist()\n",
        "  column_contents\n",
        "  # Write the column contents to a text file\n",
        "  with open('captions.txt', 'w', encoding='utf-8') as f:\n",
        "    for item in column_contents:\n",
        "        item = item.encode('utf-8').decode('unicode_escape')\n",
        "        f.write(\"%s\\n\" % item)\n",
        "# csv_to_txt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTKljzpmsmzk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCrDPHIZSZwe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import einops\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pathlib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXRrxDnxBe88"
      },
      "outputs": [],
      "source": [
        "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Pf0kfLSfFw",
        "outputId": "fb8def5e-9387-4064-c1c4-3c8fe7ad2ff9"
      },
      "outputs": [],
      "source": [
        "path_to_file = pathlib.Path('./bookstxt.txt')\n",
        "# path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
        "DATA_LIMIT = 5\n",
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "  lines = text.splitlines()\n",
        "  print(type(lines))\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "  pairs = pairs[:DATA_LIMIT]\n",
        "  # text = np.array(pairs)\n",
        "\n",
        "  text = np.array([target for target in pairs])\n",
        "\n",
        "  return text\n",
        "\n",
        "data = load_data(path_to_file)\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, 0]\n",
        "print(data.shape)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MnaJdnRSqPM",
        "outputId": "544ce993-32be-499a-e784-ba6f8fb33ce8"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TD1UFfRS7Kb"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(data)\n",
        "BATCH_SIZE = 16\n",
        "is_train = np.random.uniform(size=(len(data),)) < 0.8\n",
        "train_raw = tf.data.Dataset.from_tensor_slices(data[is_train]).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "val_raw = tf.data.Dataset.from_tensor_slices(data[~is_train] ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o62iyD_IGKWW",
        "outputId": "de36652b-6e5c-455d-bef5-a366eee14b83"
      },
      "outputs": [],
      "source": [
        "len(train_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ak5OdqfTrH_",
        "outputId": "9ce9f3e0-fc44-452c-eeae-4daab195ab14"
      },
      "outputs": [],
      "source": [
        "for context_string in train_raw.take(1):\n",
        "  print(context_string[:])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEIb-8WzXBH_",
        "outputId": "c52c1605-3416-4bf0-cc86-1b5ceceeedf7"
      },
      "outputs": [],
      "source": [
        "example_text = tf.constant(\"You have been invited to think of the two systems as agents within the mind.\")\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDW7o5jjXziy"
      },
      "outputs": [],
      "source": [
        "@tf.keras.utils.register_keras_serializable(package='Custom', name=None)\n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # add space arround punctuation\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # remove non-desplayable characters\n",
        "  text = tf.strings.regex_replace(text, '[^\\x00-\\x7F]+', '')\n",
        "  #Strip white space\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxC8leRiYUrP",
        "outputId": "bc8992fa-c258-473e-a6d2-dadfee40f731"
      },
      "outputs": [],
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEKR0cSGYXdp"
      },
      "outputs": [],
      "source": [
        "#Text Vectorization for the context text data (spainish)\n",
        "max_vocab_size = 30000\n",
        "context_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhe-V3qHbLFL",
        "outputId": "43fc33d0-28bb-4ecd-d4b1-a79567a17fe5"
      },
      "outputs": [],
      "source": [
        "context_text_processor.adapt(train_raw)\n",
        "print(context_text_processor.get_vocabulary()[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pk.dump(context_text_processor.get_config(), open('text_processor_config.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhXyFmFabmrG",
        "outputId": "5a45031e-48b0-4f4b-df59-fa0920b761ac"
      },
      "outputs": [],
      "source": [
        "# now layers can convert batch of strings to batch of token ids\n",
        "example_tokens = context_text_processor(context_string)\n",
        "print(example_tokens[:3, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzyL30lHcKHj",
        "outputId": "17107639-a6bf-4194-82c9-7b6421da6083"
      },
      "outputs": [],
      "source": [
        "#The get_vocabulary method can be used to convert token IDs back to text:\n",
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "tokens = ' '.join(tokens)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj7hohYnckrp",
        "outputId": "bec739e3-8760-4423-bb77-2df06127a872"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_text(context):\n",
        "  target = context_text_processor(context)\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  # print(type(context))\n",
        "  # targ_in = target[:, :-1].to_tensor() #take everthing in axiz = 0 and take everything except the last in axis = 2\n",
        "  # targ_in = target[:, 1:].to_tensor()\n",
        "  # targ_out = target[:, :-1].to_tensor()\n",
        "  target = target[:, 1:]\n",
        "  targ_in = target[:, :-1].to_tensor() #take everthing in axiz = 0 and take everything except the last in axis = 2\n",
        "  targ_out = target[:, 1:].to_tensor()\n",
        "\n",
        "  \n",
        "\n",
        "  print('process_text')\n",
        "\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnWTGDyDEjBr",
        "outputId": "bd240788-a004-462d-b8a1-0e094a76c5ef"
      },
      "outputs": [],
      "source": [
        "len(train_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WJihLzsdCao",
        "outputId": "2c93be65-d814-4553-d589-1af8ceba6adb"
      },
      "outputs": [],
      "source": [
        "for (example_context_tokens, target_in_tokens), target_out_tokens in train_ds.take(2):\n",
        "\n",
        "  print(example_context_tokens[0, :10].numpy(), example_context_tokens[0, :].numpy().shape)\n",
        "  print(target_in_tokens[0, :10].numpy(), target_in_tokens[0, :].numpy().shape)\n",
        "  print(target_out_tokens[0, :10].numpy(), target_out_tokens[0, :].numpy().shape)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4lUixvWV0eL"
      },
      "outputs": [],
      "source": [
        "# target_in_tokens = target_in_tokens + 1\n",
        "# print(target_in_tokens[0, :10].numpy(),'\\n', target_out_tokens[0, :10].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gccz0Pib9hcj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8BHQKIedKKL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The encoder:\n",
        "  1. Takes a list of token IDs (from context_text_processor).\n",
        "  2. Looks up an embedding vector for each token (Using a layers.Embedding).\n",
        "  3. Processes the embeddings into a new sequence (Using a bidirectional layers.GRU).\n",
        "  4. Returns the processed sequence. This will be passed to the attention head.\n",
        "'''\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "\n",
        "    #The embedding layer converts tokens into vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.units, mask_zero=True)\n",
        "\n",
        "    #The RNN layer processes those vectors sequentially\n",
        "    self.rnn = tf.keras.layers.Bidirectional(merge_mode='sum', \n",
        "                                             layer=tf.keras.layers.GRU(self.units, return_sequences=True, recurrent_initializer='glorot_uniform' ))\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.rnn(x)\n",
        "    # print('call')\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLCMOwS5dZ5C"
      },
      "outputs": [],
      "source": [
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shfWDGBPdb58",
        "outputId": "208acf52-57f4-4079-92fc-1ada0c870c47"
      },
      "outputs": [],
      "source": [
        "# Encode the input sequence.\n",
        "encoder = Encoder(context_text_processor, UNITS)\n",
        "ex_context = encoder(example_context_tokens)\n",
        "\n",
        "print(f'Context tokens, shape (batch, s): {example_context_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7x-Gtg8dd0g"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    atten_output, atten_score = self.mha(query=x, value=context, return_attention_scores=True)\n",
        "    x = self.add([x, atten_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mET4qJxxdyMG"
      },
      "outputs": [],
      "source": [
        "attention_layer = CrossAttention(UNITS)\n",
        "# Attend to the encoded tokens\n",
        "embed = tf.keras.layers.Embedding(context_text_processor.vocabulary_size(),\n",
        "                                  output_dim=UNITS, mask_zero=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vNl2zVLd0Wq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
        "  1. It looks up embeddings for each token in the target sequence.\n",
        "  2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
        "  3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
        "  4. At each location in the output it predicts the next token.\n",
        "'''\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.units, mask_zero=True)\n",
        "    # 2. The RNN keeps track of what's been generated so far.\n",
        "    self.rnn = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    #3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(self.units)\n",
        "\n",
        "    # self.fc1 = tf.keras.layers.Dense(self.units, activation='relu')\n",
        "    # 4. This fully connected layer produces the logits for each output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "  def call(self, context, x, state=None, return_state=False):\n",
        "    #Lookup for embeddings\n",
        "    x = self.embedding(x)\n",
        "    #Process the target sequence\n",
        "    x, state = self.rnn(x, initial_state=state)\n",
        "    #Use the rnn output as the query for the attention over the context\n",
        "    x = self.attention(x, context)\n",
        "\n",
        "    # x = self.fc1(x)\n",
        "\n",
        "    #generate logit predictyions for the next token\n",
        "    logits = self.output_layer(x)\n",
        "\n",
        "\n",
        "    if return_state:\n",
        "      return logits, state\n",
        "    return logits\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGJhPEvRd-eC"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(context_text_processor, UNITS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj2NEDLZeBd-"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  print(batch_size)\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  # print(start_tokens, 'satrt token')\n",
        "  # print(tf.constant(context), 'context')\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "\n",
        "# print(example_context_tokens, 'ex_context_tokens')\n",
        "# ex_context = encoder(example_context_tokens)\n",
        "# next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "# print(next_token, state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBmT2Ia2eHjd"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result\n",
        "\n",
        "# decoder.tokens_to_text(example_context_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jwSPALseI8x"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "\n",
        "  logits, state = self(context, next_token, state = state, return_state=True) \n",
        "\n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If a sequence produces an `end_token`, set it `done`\n",
        "  # done = done | (next_token == self.end_token)\n",
        "  # # Once a sequence is done it only produces 0-padding.\n",
        "  # next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "  return next_token, done, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyF377dDeKge",
        "outputId": "78cc582a-479d-4a40-e88a-2d8f6327d608"
      },
      "outputs": [],
      "source": [
        "# Setup the loop variables.\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "for n in range(10):\n",
        "  # Run one step.\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Add the token to the output.\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Stack all the tokens together.\n",
        "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
        "\n",
        "# Convert the tokens back to a a string\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "result[:3].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBokFp4beNDI"
      },
      "outputs": [],
      "source": [
        "class TextGen(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units, context_text_processor, target_text_processor):\n",
        "    super().__init__()\n",
        "    #Build the encoder and the decoder\n",
        "    self.encoder = Encoder(context_text_processor, units)\n",
        "    self.decoder = Decoder(context_text_processor, units)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # print(\"here1\")\n",
        "    context, x = inputs\n",
        "    # print(\"here2\")\n",
        "\n",
        "    context = self.encoder(context)\n",
        "    # print(\"here3\")\n",
        "\n",
        "    logits = self.decoder(context, x)\n",
        "    # print(\"here4\")\n",
        "\n",
        "    try:\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    # print(\"here5\", logits)\n",
        "    return logits\n",
        "  \n",
        "  def gen(self,\n",
        "                texts, *,\n",
        "                max_length=50,\n",
        "                temperature=0.0):\n",
        "    # Process the input texts\n",
        "    context = self.encoder.convert_input(texts)\n",
        "    batch_size = tf.shape(texts)[0]\n",
        "\n",
        "    # Setup the loop inputs\n",
        "    tokens = []\n",
        "    attention_weights = []\n",
        "    next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      # Generate the next token\n",
        "      next_token, done, state = self.decoder.get_next_token(context, next_token, done,  state, temperature)\n",
        "\n",
        "      # Collect the generated tokens\n",
        "      tokens.append(next_token)\n",
        "      # attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "      # if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      #   break\n",
        "\n",
        "    # Stack the lists of tokens and attention weights.\n",
        "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "    # self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "    result = self.decoder.tokens_to_text(tokens)\n",
        "    return result\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbeaZcS3edil"
      },
      "outputs": [],
      "source": [
        "model = TextGen(UNITS, context_text_processor, context_text_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Ea88nwehHJ"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    # print(y_true.shape, y_pred.shape, \"masked_loss\")\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "    \n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWY7CEP5ejoz"
      },
      "outputs": [],
      "source": [
        "#custom accuracy\n",
        "def masked_acc(y_true, y_pred):\n",
        "  # Calculate the loss for each item in the batch\n",
        "  y_pred = tf.argmax(y_pred, axis=-1)\n",
        "  y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "  match = tf.cast(y_true == y_pred, dtype=tf.float32)\n",
        "  mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "  return tf.reduce_sum(match) / tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2NwmkorelRF"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPBGhwpWenSV",
        "outputId": "dc0f453f-82a2-4a7e-f6c6-047dbfba0dbd"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1.0 * context_text_processor.vocabulary_size()\n",
        "\n",
        "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        " \"expected_acc\": 1/vocab_size}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTowsTJZepLE"
      },
      "outputs": [],
      "source": [
        "# model.evaluate(val_ds, steps=20, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFjl0XSdetaj",
        "outputId": "4d199a78-69fa-417b-8791-3c5bbaa06517"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(), \n",
        "    epochs=10,\n",
        "    steps_per_epoch = 20,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps = 20,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, monitor='masked_acc')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouSULgTjSGAl"
      },
      "outputs": [],
      "source": [
        "result = model.gen(['This was one of my favorite'], max_length=100)\n",
        "result = result[0].numpy().decode()\n",
        "print(result)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAVE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = f'{DATA_LIMIT}_model.tf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP2ZseJ_SKmG"
      },
      "outputs": [],
      "source": [
        "#save the vectorization layer\n",
        "pickle.dump(context_text_processor.get_config(), open('text_processor_config.pkl', 'wb'))\n",
        "weights = context_text_processor.get_weights()\n",
        "text_processor_weights = pickle.dump(weights, open(\"text_processor_weights.pkl\", \"wb\"))\n",
        "#save the model\n",
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_model = None\n",
        "loaded_model = tf.keras.models.load_model(model_path, custom_objects={'tf_lower_and_split_punct': tf_lower_and_split_punct}, compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = loaded_model.predict(['This was one of my favorite'])\n",
        "result = result[0].numpy().decode()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# weights = context_text_processor.get_weights()\n",
        "\n",
        "# text_processor_weights = pickle.dump(weights, open(\"text_processor_weights.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "loaded_config = pickle.load(open(\"text_processor_config.pkl\", 'rb'))\n",
        "loaded_weights = pickle.load(open(\"text_processor_weights.pkl\", 'rb'))\n",
        "text_processor = tf.keras.layers.TextVectorization.from_config(loaded_config)\n",
        "text_processor.set_weights(loaded_weights)\n",
        "\n",
        "new_model = TextGen(UNITS, text_processor, text_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_model.load_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = new_model.gen(['Hello world'])\n",
        "result = result[0].numpy().decode()\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(path_to_model, path_to_vectorizer_config, path_to_vectorizer_weights):\n",
        "    loaded_config = pickle.load(open(path_to_vectorizer_config, 'rb'))\n",
        "    loaded_weights = pickle.load(open(path_to_vectorizer_weights, 'rb'))\n",
        "\n",
        "    text_processor = tf.keras.layers.TextVectorization.from_config(loaded_config)\n",
        "    text_processor.set_weights(loaded_weights)\n",
        "\n",
        "    new_model = TextGen(UNITS, text_processor, text_processor)\n",
        "\n",
        "    return new_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "textGenModel = create_model(path_to_model=model_path, path_to_vectorizer_config='text_processor_config.pkl', path_to_vectorizer_weights='text_processor_weights.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1,), dtype=string, numpy=\n",
              "array([b'doubly last busy quirk had subtle doing really wondering , noses piled solution his most of see everybody im of see everybody im of see everybody whiskey the health about know depression thing alcoholic charles victims not poet poet very very poet very poet very poet very poet very poet'],\n",
              "      dtype=object)>"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "textGenModel.gen(['Hello World'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
