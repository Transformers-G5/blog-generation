{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj0e8obvNSmO",
        "outputId": "850f5658-b92b-497e-83ed-5318669ad631"
      },
      "outputs": [],
      "source": [
        "# !pip install \"tensorflow-text\"\n",
        "# !pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K6Ep2oKhNaH0"
      },
      "outputs": [],
      "source": [
        "#EXP\n",
        "# import pandas as pd\n",
        "# convert csv to txt\n",
        "def csv_to_txt( path = '/content/captions_csv.csv'):\n",
        "  import pandas as pd\n",
        "  data = pd.read_csv(path)\n",
        "  data.dropna()\n",
        "  print(data.head())\n",
        "  column_contents = data['Caption'].astype(str).values.tolist()\n",
        "  column_contents\n",
        "  # Write the column contents to a text file\n",
        "  with open('captions.txt', 'w', encoding='utf-8') as f:\n",
        "    for item in column_contents:\n",
        "        item = item.encode('utf-8').decode('unicode_escape')\n",
        "        f.write(\"%s\\n\" % item)\n",
        "# csv_to_txt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DTKljzpmsmzk"
      },
      "outputs": [],
      "source": [
        "#CONSTANTS\n",
        "path_to_data_file = './bookstxt.txt'\n",
        "DATA_LIMIT = 40\n",
        "model_root_path = '../models/attention'\n",
        "model_name = 'fc1'\n",
        "model_path = f'{model_root_path}/{DATA_LIMIT}_model_{model_name}'\n",
        "vectorizer_config_path = '../models/attention/text_processor_config.pkl'\n",
        "vectorizer_weights_path = '../models/attention/text_processor_weights.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DCrDPHIZSZwe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import einops\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.ticker as ticker\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pathlib\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jXRrxDnxBe88"
      },
      "outputs": [],
      "source": [
        "# path_to_zip = tf.keras.utils.get_file('spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01Pf0kfLSfFw",
        "outputId": "fb8def5e-9387-4064-c1c4-3c8fe7ad2ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "(40, 1)\n",
            "(40,)\n",
            "['Charles Bukowski was an alcoholic, a womanizer, a chronic gambler, a lout, a cheapskate, a deadbeat, and on his worst days, a poet. He’s probably the last person on earth you would ever look to for life advice or expect to see in any sort of self-help book. Which is why he’s the perfect place to start. Bukowski wanted to be a writer. But for decades his work was rejected by almost every magazine, newspaper, journal, agent, and publisher he submitted to. His work was horrible, they said. Crude. Disgusting. Depraved. And as the stacks of rejection slips piled up, the weight of his failures pushed him deep into an alcohol-fueled depression that would follow him for most of his life. Bukowski had a day job as a letter-filer at a post office. He got paid shit money and spent most of it on booze. He gambled away the rest at the racetrack. At night, he would drink alone and sometimes hammer out poetry on his beat-up old typewriter. Often, he’d wake up on the floor, having passed out the night befo...'\n",
            " 'There’s an insidious quirk to your brain that, if you let it, can drive you absolutely batty. Tell me if this sounds familiar to you: You get anxious about confronting somebody in your life. That anxiety cripples you and you start wondering why you’re so anxious. Now you’re becoming anxious about being anxious. Oh no! Doubly anxious! Now you’re anxious about your anxiety, which is causing more anxiety. Quick, where’s the whiskey? Or let’s say you have an anger problem. You get pissed off at the stupidest, most inane stuff, and you have no idea why. And the fact that you get pissed off so easily starts to piss you off even more. And then, in your petty rage, you realize that being angry all the time makes you a shallow and mean person, and you hate this; you hate it so much that you get angry at yourself. Now look at you: you’re angry at yourself getting angry about being angry. Fuck you, wall. Here, have a fist. Or you’re so worried about doing the right thing all the time that you become wor...'\n",
            " '’s always been that way, and always will be. By not giving a fuck that you feel bad, you short-circuit the Feedback Loop from Hell; you say to yourself, “I feel like shit, but who gives a fuck?” And then, as if sprinkled by magic fuck-giving fairy dust, you stop hating yourself for feeling so bad. George Orwell said that to see what’s in front of one’s nose requires a constant struggle. Well, the solution to our stress and anxiety is right there in front of our noses, and we’re too busy watching porn and advertisements for ab machines that don’t work, wondering why we’re not banging a hot blonde with a rocking six-pack, to notice. We joke online about “first-world problems,” but we really have become victims of our own success. Stress-related health issues, anxiety disorders, and cases of depression have skyrocketed over the past thirty years, despite the fact that everyone has a flat-screen TV and can have their groceries delivered. Our crisis is no longer material; it’s existential, it’s spiritual. We ha...'\n",
            " 'we’re getting pissed off about nickels and Everybody Loves Raymond. Look, this is how it works. You’re going to die one day. I know that’s kind of obvious, but I just wanted to remind you in case you’d forgotten. You and everyone you know are going to be dead soon. And in the short amount of time between here and there, you have a limited amount of fucks to give. Very few, in fact. And if you go around giving a fuck about everything and everyone without conscious thought or choice—well, then you’re going to get fucked. There is a subtle art to not giving a fuck. And though the concept may sound ridiculous and I may sound like an asshole, what I’m talking about here is essentially learning how to focus and prioritize your thoughts effectively—how to pick and choose what matters to you and what does not matter to you based on finely honed personal values. This is incredibly difficult. It takes a lifetime of practice and discipline to achieve. And you will regularly fail. But it is perhaps the most worthy str...'\n",
            " 'question, then, is, What do we give a fuck about? What are we choosing to give a fuck about? And how can we not give a fuck about what ultimately does not matter? My mother was recently screwed out of a large chunk of money by a close friend of hers. Had I been indifferent, I would have shrugged my shoulders, sipped my mocha, and downloaded another season of The Wire. Sorry, Mom. But instead, I was indignant. I was pissed off. I said, “No, screw that, Mom. We’re going to lawyer the fuck up and go after this asshole. Why? Because I don’t give a fuck. I will ruin this guy’s life if I have to.” This illustrates the first subtlety of not giving a fuck. When we say, “Damn, watch out, Mark Manson just don’t give a fuck,” we don’t mean that Mark Manson doesn’t care about anything; on the contrary, we mean that Mark Manson doesn’t care about adversity in the face of his goals, he doesn’t care about pissing some people off to do what he feels is right or important or noble. We mean that Mark Manson is the type of ...'\n",
            " 'People aren’t just born not giving a fuck. In fact, we’re born giving way too many fucks. Ever watch a kid cry his eyes out because his hat is the wrong shade of blue? Exactly. Fuck that kid. When we’re young, everything is new and exciting, and everything seems to matter so much. Therefore, we give tons of fucks. We give a fuck about everything and everyone—about what people are saying about us, about whether that cute boy/girl called us back or not, about whether our socks match or not, or what color our birthday balloon is. As we get older, with the benefit of experience (and having seen so much time slip by), we begin to notice that most of these sorts of things have little lasting impact on our lives. Those people whose opinions we cared about so much before are no longer present in our lives. Rejections that were painful in the moment have actually worked out for the best. We realize how little attention people pay to the superficial details about us, and we choose not to obsess so much over them. E...'\n",
            " 'This book will help you think a little bit more clearly about what you’re choosing to find important in life and what you’re choosing to find unimportant. I believe that today we’re facing a psychological epidemic, one in which people no longer realize it’s okay for things to suck sometimes. I know that sounds intellectually lazy on the surface, but I promise you, it’s a life/death sort of issue. Because when we believe that it’s not okay for things to suck sometimes, then we unconsciously start blaming ourselves. We start to feel as though something is inherently wrong with us, which drives us to all sorts of overcompensation, like buying forty pairs of shoes or downing Xanax with a vodka chaser on a Tuesday night or shooting up a school bus full of kids. This belief that it’s not okay to be inadequate sometimes is the source of the growing Feedback Loop from Hell that is coming to dominate our culture. The idea of not giving a fuck is a simple way of reorienting our expectations for life...'\n",
            " 'About twenty-five hundred years ago, in the Himalayan foothills of present-day Nepal, there lived in a great palace a king who was going to have a son. For this son the king had a particularly grand idea: he would make the child’s life perfect. The child would never know a moment of suffering—every need, every desire, would be accounted for at all times. The king built high walls around the palace that prevented the prince from knowing the outside world. He spoiled the child, lavishing him with food and gifts, surrounding him with servants who catered to his every whim. And just as planned, the child grew up ignorant of the routine cruelties of human existence. All of the prince’s childhood went on like this. But despite the endless luxury and opulence, the prince became kind of a pissed-off young man. Soon, every experience felt empty and valueless. The problem was that no matter what his father gave him, it never seemed enough, never meant anything. So late one night, the prince snuck out of the palace...'\n",
            " 'If I could invent a superhero, I would invent one called Disappointment Panda. He’d wear a cheesy eye mask and a shirt (with a giant capital T on it) that was way too small for his big panda belly, and his superpower would be to tell people harsh truths about themselves that they needed to hear but didn’t want to accept. He would go door-to-door like a Bible salesman and ring doorbells and say things like, “Sure, making a lot of money makes you feel good, but it won’t make your kids love you,” or “If you have to ask yourself if you trust your wife, then you probably don’t,” or “What you consider ‘friendship’ is really just your constant attempts to impress people.” Then he’d tell the homeowner to have a nice day and saunter on down to the next house. It would be awesome. And sick. And sad. And uplifting. And necessary. After all, the greatest truths in life are usually the most unpleasant to hear. Disappointment Panda would be the hero that none of us would want but all of us would need. He’d be the prove...'\n",
            " 'Problems are a constant in life. When you solve your health problem by buying a gym membership, you create new problems, like having to get up early to get to the gym on time, sweating like a meth- head for thirty minutes on an elliptical, and then getting showered and changed for work so you don’t stink up the whole office. When you solve your problem of not spending enough time with your partner by designating Wednesday night “date night,” you generate new problems, such as figuring out what to do every Wednesday that you both won’t hate, making sure you have enough money for nice dinners, rediscovering the chemistry and spark you two feel you’ve lost, and unraveling the logistics of fucking in a small bathtub filled with too many bubbles. Problems never stop; they merely get exchanged and/or upgraded. Happiness comes from solving problems. The keyword here is “solving.” If you’re avoiding your problems or feel like you don’t have any problems, then you’re going to make yourself miserable. If you feel l...'\n",
            " 'Emotions evolved for one specific purpose: to help us live and reproduce a little bit better. That’s it. They’re feedback mechanisms telling us that something is either likely right or likely wrong for us— nothing more, nothing less. Much as the pain of touching a hot stove teaches you not to touch it again, the sadness of being alone teaches you not to do the things that made you feel so alone again. Emotions are simply biological signals designed to nudge you in the direction of beneficial change. Look, I don’t mean to make light of your midlife crisis or the fact that your drunk dad stole your bike when you were eight years old and you still haven’t gotten over it, but when it comes down to it, if you feel crappy it’s because your brain is telling you that there’s a problem that’s unaddressed or unresolved. In other words, negative emotions are a call to action. When you feel them, it’s because you’re supposed to do something. Positive emotions, on the other hand, are rewards for taking the proper acti...'\n",
            " 'If I ask you, “What do you want out of life?” and you say something like, “I want to be happy and have a great family and a job I like,” your response is so common and expected that it doesn’t really mean anything. Everybody enjoys what feels good. Everyone wants to live a carefree, happy, and easy life, to fall in love and have amazing sex and relationships, to look perfect and make money and be popular and well-respected and admired and a total baller to the point that people part like the Red Sea when they walk into the room. Everybody wants that. It’s easy to want that. A more interesting question, a question that most people never consider, is, “What pain do you want in your life? What are you willing to struggle for?” Because that seems to be a greater determinant of how our lives turn out. For example, most people want to get the corner office and make a boatload of money—but not many people want to suffer through sixty-hour workweeks, long commutes, obnoxious paperwork, and arbitr...'\n",
            " 'I once knew a guy; we’ll call him Jimmy. Jimmy always had various business ventures going. On any given day, if you asked him what he was doing, he’d rattle off the name of some firm he was consulting with, or he’d describe a promising medical app he was looking for angel investors to fund, or he’d talk about some charity event he was supposed to be the keynote speaker for, or how he had an idea for a more efficient type of gas pump that was going to make him billions. The guy was always rolling, always on, and if you gave him an inch of conversational daylight, he’d pulverize you about how world-spinning his work was, how brilliant his latest ideas were, and he’d name-drop so much it felt like you were talking to a tabloid reporter. Jimmy was all positivity all the time. Always pushing himself, always working an angle—a real go-getter, whatever the fuck that means. The catch was that Jimmy was also a total deadbeat—all talk and no walk. Stoned a majority of the time, and spending as muc...'\n",
            " 'I sat in my 9:00 A.M. biology class, arms cradling my head on my desk as I stared at the clock’s second hand making laps, each tick syncopated with the teacher’s dronings-on about chromosomes and mitosis. Like most thirteen-year-olds stuck in a stuffy, fluorescent classroom, I was bored. A knock came on the door. Mr. Price, the school’s assistant principal, stuck his head in. “Excuse me for interrupting. Mark, can you step outside with me for a moment? Oh, and bring your things with you.” Strange, I thought. Kids get sent to the principal, but the principal rarely gets sent to them. I gathered my things and stepped out. The hallway was empty. Hundreds of beige lockers converged on the horizon. “Mark, can you take me to your locker, please?” “Sure,” I say, and slug myself down the hall, baggy jeans and moppy hair and oversized Pantera T-shirt and all. We get to my locker. “Open it, please,” Mr. Price says; so I do. He steps in front of me and gathers my coat, my gym bag, my backpack—all of the ...'\n",
            " 'Most of us are pretty average at most things we do. Even if you’re exceptional at one thing, chances are you’re average or below average at most other things. That’s just the nature of life. To become truly great at something, you have to dedicate shit-tons of time and energy to it. And because we all have limited time and energy, few of us ever become truly exceptional at more than one thing, if anything at all. We can then say that it’s a statistical improbability that any single person will be an extraordinary performer in all areas of life, or even in many areas of their life. Brilliant businesspeople are often fuckups in their personal lives. Extraordinary athletes are often shallow and as dumb as a lobotomized rock. Many celebrities are probably just as clueless about life as the people who gawk at them and follow their every move. We’re all, for the most part, pretty average people. But it’s the extremes that get all of the publicity. We kind of know this already, but...'\n",
            " 'ebook, YouTube, and access to five hundred–plus channels of television is amazing. But our attention is limited. There’s no way we can process the tidal waves of information flowing past us constantly. Therefore, the only zeroes and ones that break through and catch our attention are the truly exceptional pieces of information—those in the 99.999th percentile. All day, every day, we are flooded with the truly extraordinary. The best of the best. The worst of the worst. The greatest physical feats. The funniest jokes. The most upsetting news. The scariest threats. Nonstop. Our lives today are filled with information from the extremes of the bell curve of human experience, because in the media business that’s what gets eyeballs, and eyeballs bring dollars. That’s the bottom line. Yet the vast majority of life resides in the humdrum middle. The vast majority of life is unextraordinary, indeed quite average. This flood of extreme information has conditioned us to believe that exceptionalism is t...'\n",
            " 'In the closing months of 1944, after almost a decade of war, the tide was turning against Japan. Their economy was floundering, their military overstretched across half of Asia, and the territories they had won throughout the Pacific were now toppling like dominoes to U.S. forces. Defeat seemed inevitable. On December 26, 1944, Second Lieutenant Hiroo Onoda of the Japanese Imperial Army was deployed to the small island of Lubang in the Philippines. His orders were to slow the United States’ progress as much as possible, to stand and fight at all costs, and to never surrender. Both he and his commander knew it was essentially a suicide mission. In February 1945, the Americans arrived on Lubang and took the island with overwhelming force. Within days, most of the Japanese soldiers had either surrendered or been killed, but Onoda and three of his men managed to hide in the jungle. From there, they began a guerrilla warfare campaign against the U.S. forces and the local population, attacking supply lines, sh...'\n",
            " 'Self-awareness is like an onion. There are multiple layers to it, and the more you peel them back, the more likely you’re going to start crying at inappropriate times. Let’s say the first layer of the self-awareness onion is a simple understanding of one’s emotions. “This is when I feel happy.” “This makes me feel sad.” “This gives me hope.” Unfortunately, there are many people who suck at even this most basic level of self-awareness. I know because I’m one of them. My wife and I sometimes have a fun back-and-forth that goes something like this: HER. What’s wrong? ME. Nothing’s wrong. Nothing at all. HER. No, something’s wrong. Tell me. ME. I’m fine. Really. HER. Are you sure? You look upset. ME, with nervous laughter. Really? No, I’m okay, seriously. [Thirty minutes later . . . ] ME. . . . And that’s why I’m so fucking pissed off! He just acts as if I don’t exist half the time. We all have emotional blind spots. Often they have to do with the emotions that we were taught were inappropriate...'\n",
            " 'In 1983, a talented young guitarist was kicked out of his band in the worst possible way. The band had just been signed to a record deal, and they were about to record their first album. But a couple days before recording began, the band showed the guitarist the door—no warning, no discussion, no dramatic blowout; they literally woke him up one day by handing him a bus ticket home. As he sat on the bus back to Los Angeles from New York, the guitarist kept asking himself: How did this happen? What did I do wrong? What will I do now? Record contracts didn’t exactly fall out of the sky, especially for raucous, upstart metal bands. Had he missed his one and only shot? But by the time the bus hit L.A., the guitarist had gotten over his self-pity and had vowed to start a new band. He decided that this new band would be so successful that his old band would forever regret their decision. He would become so famous that they would be subjected to decades of seeing him on TV, hearing him on the radio, seeing poster...'\n",
            " 'There are a handful of common values that create really poor problems for people—problems that can hardly be solved. So let’s go over some of them quickly: 1. Pleasure. Pleasure is great, but it’s a horrible value to prioritize your life around. Ask any drug addict how his pursuit of pleasure turned out. Ask an adulterer who shattered her family and lost her children whether pleasure ultimately made her happy. Ask a man who almost ate himself to death how pleasure helped him solve his problems. Pleasure is a false god. Research shows that people who focus their energy on superficial pleasures end up more anxious, more emotionally unstable, and more depressed. Pleasure is the most superficial form of life satisfaction and therefore the easiest to obtain andthe easiest to lose. And yet, pleasure is what’s marketed to us, twenty-four/seven. It’s what we fixate on. It’s what we use to numb and distract ourselves. But pleasure, while necessary in life (in certain doses), isn’t, ...'\n",
            " 'Good values are 1) reality-based, 2) socially constructive, and 3) immediate and controllable. Bad values are 1) superstitious, 2) socially destructive, and 3) not immediate or controllable. Honesty is a good value because it’s something you have complete control over, it reflects reality, and it benefits others (even if it’s sometimes unpleasant). Popularity, on the other hand, is a bad value. If that’s your value, and if your metric is being the most popular guy/girl at the dance party, much of what happens will be out of your control: you don’t know who else will be at the event, and you probably won’t know who half those people are. Second, the value/metric isn’t based on reality: you may feel popular or unpopular, when in fact you have no fucking clue what anybody else really thinks about you. (Side Note: As a rule, people who are terrified of what others think about them are actually terrified of all the shitty things they think about themselves being reflected back at them.) Some examples of goo...'\n",
            " 'William James had problems. Really bad problems. Although born into a wealthy and prominent family, from birth James suffered life-threatening health issues: an eye problem that left him temporarily blinded as a child; a terrible stomach condition that caused excessive vomiting and forced him to adopt an obscure and highly sensitive diet; trouble with his hearing; back spasms so bad that for days at a time he often couldn’t sit or stand upright. Due to his health problems, James spent most of his time at home. He didn’t have many friends, and he wasn’t particularly good at school. Instead, he passed the days painting. That was the only thing he liked and the only thing he felt particularly good at. Unfortunately, nobody else thought he was good at it. When he grew to adulthood, nobody bought his work. And as the years dragged on, his father (a wealthy businessman) began ridiculing him for his laziness and his lack of talent. Meanwhile, his younger brother, Henry James, went on to become a world-renowned n...'\n",
            " 'Years ago, when I was much younger and stupider, I wrote a blog post, and at the end of it I said something like, “And as a great philosopher once said: ‘With great power comes great responsibility.’” It sounded nice and authoritative. I couldn’t remember who had said it, and my Google search had turned up nothing, but I stuck it in there anyway. It fit the post nicely. About ten minutes later, the first comment came in: “I think the ‘great philosopher’ you’re referring to is Uncle Ben from the movie Spider-Man.” As another great philosopher once said, “Doh!” “With great power comes great responsibility.” The last words of Uncle Ben before a thief whom Peter Parker let get away murders him on a sidewalk full of people for absolutely no explicable reason. That great philosopher. Still, we’ve all heard the quote. It gets repeated a lot—usually ironically and after about seven beers. It’s one of those perfect quotes that sound really intelligent, and yet it’s basically just telling...'\n",
            " 'But what about really awful events? A lot of people can get on board with taking responsibility for work-related problems and maybe watching too much TV when they should really be playing with their kids or being productive. But when it comes to horrible tragedies, they pull the emergency cord on the responsibility train and get off when it stops. Some things just feel too painful for them to own up to. But think about it: the intensity of the event doesn’t change the underlying truth. If you get robbed, say, you’re obviously not at fault for being robbed. No one would ever choose to go through that. But as with the baby on your doorstep, you are immediately thrust into responsibility for a life-and-death situation. Do you fight back? Do you panic? Do you freeze up? Do you tell the police? Do you try to forget it and pretend it never happened? These are all choices and reactions you’re responsible for making or rejecting. You didn’t choose the robbery, but it’s still your responsibility to manage the emot...'\n",
            " 'In 2013, the BBC rounded up half a dozen teenagers with obsessive-compulsive disorder (OCD) and followed them as they attended intensive therapies to help them overcome their unwanted thoughts and repetitive behaviors. There was Imogen, a seventeen-year-old girl who had a compulsive need to tap every surface she walked past; if she failed to do so, she was flooded with horrible thoughts of her family dying. There was Josh, who needed to do everything with both sides of his body—shake a person’s hand with both his right and his left hand, eat his food with each hand, step through a doorway with both feet, and so on. If he didn’t “equalize” his two sides, he suffered from severe panic attacks. And then there was Jack, a classic germophobe who refused to leave his house without wearing gloves, boiled all his water before drinking it, and refused to eat food not cleaned and prepared himself. OCD is a terrible neurological and genetic disorder that cannot be cured. At best, it can be managed. A...'\n",
            " 'The responsibility/fault fallacy allows people to pass off the responsibility for solving their problems to others. This ability to alleviate responsibility through blame gives people a temporary high and a feeling of moral righteousness. Unfortunately, one side effect of the Internet and social media is that it’s become easier than ever to push responsibility—for even the tiniest of infractions—onto some other group or person. In fact, this kind of public blame/shame game has become popular; in certain crowds it’s even seen as “cool.” The public sharing of “injustices” garners far more attention and emotional outpouring than most other events on social media, rewarding people who are able to perpetually feel victimized with ever- growing amounts of attention and sympathy. “Victimhood chic” is in style on both the right and the left today, among both the rich and the poor. In fact, this may be the first time in human history that every single demographic group has felt unfairly victimized simultaneously. ...'\n",
            " 'A lot of people might hear all of this and then say something like, “Okay, but how? I get that my values suck and that I avoid responsibility for all of my problems and that I’m an entitled little shit who thinks the world should revolve around me and every inconvenience I experience—but how do I change?” And to this I say, in my best Yoda impersonation: “Do, or do not; there is no ‘how.’ ” You are already choosing, in every moment of every day, what to give a fuck about, so change is as simple as choosing to give a fuck about something else. It really is that simple. It’s just not easy. It’s not easy because you’re going to feel like a loser, a fraud, a dumbass at first. You’re going to be nervous. You’re going to freak out. You may get pissed off at your wife or your friends or your father in the process. These are all side effects of changing your values, of changing the fucks you’re giving. But they are inevitable. It’s simple but really, really hard. Let’s look at some of these side effects. You’re g...'\n",
            " 'Five hundred years ago cartographers believed that California was an island. Doctors believed that slicing a person’s arm open (or causing bleeding anywhere) could cure disease. Scientists believed that fire was made out of something called phlogiston. Women believed that rubbing dog urine on their face had anti-aging benefits. Astronomers believed that the sun revolved around the earth. When I was a little boy, I used to think “mediocre” was a kind of vegetable that I didn’t want to eat. I thought my brother had found a secret passageway in my grandmother’s house because he could get outside without having to leave the bathroom (spoiler alert: there was a window). I also thought that when my friend and his family visited “Washington, B.C.,” they had somehow traveled back in time to when the dinosaurs lived, because after all, “B.C.” was a long time ago. As a teenager, I told everybody that I didn’t care about anything, when the truth was I cared about way too much. Other people ruled my world without my...'\n",
            " 'Try this. Take a random person and put them in a room with some buttons to push. Then tell them that if they do something specific—some undefined something that they have to figure out—a light will flash on indicating that they’ve won a point. Then tell them to see how many points they can earn within a thirty-minute period. When psychologists have done this, what happens is what you might expect. People sit down and start mashing buttons at random until eventually the light comes on to tell them they got a point. Logically, they then try repeating whatever they were doing to get more points. Except now the light’s not coming on. So they start experimenting with more complicated sequences—press this button three times, then this button once, then wait five seconds, and—ding! Another point. But eventually that stops working. Perhaps it doesn’t have to do with buttons at all, they think. Perhaps it has to do with how I’m sitting. Or what I’m touching. Maybe it has to do with my feet. Ding! Another point. Ye...'\n",
            " 'In 1988, while in therapy, the journalist and feminist author Meredith Maran came to a startling realization: her father had sexually abused her as a child. It was a shock to her, a repressed memory she had spent most of her adult life oblivious to. But at the age of thirty-seven, she confronted her father and also told her family what had happened. Meredith’s news horrified her entire family. Her father immediately denied having done anything. Some family members sided with Meredith. Others sided with her father. The family tree was split in two. And the pain that had defined Meredith’s relationship with her father since long before her accusation now spread like a mold across its branches. It tore everyone apart. Then, in 1996, Meredith came to another startling realization: her father actually hadn’t sexually abused her. (I know: oops.) She, with the help of a well-intentioned therapist, had actually invented the memory. Consumed by guilt, she spent the rest of her father’s life attempting to reconcile...'\n",
            " 'Erin sits across from me at the sushi restaurant and tries to explain why she doesn’t believe in death. It’s been almost three hours, and she’s eaten exactly four cucumber rolls and drunk an entire bottle of sake by herself. (In fact, she’s about halfway through bottle number two now.) It’s four o’clock on aTuesday afternoon. I didn’t invite her here. She found out where I was via the Internet and flew out to come find me. Again. She’s done this before. You see, Erin is convinced that she can cure death, but she’s also convinced that she needs my help to do it. But not my help in like a business sense. If she just needed some PR advice or something, that would be one thing. No, it’s more than that: she needs me to be her boyfriend. Why? After three hours of questioning and a bottle and a half of sake, it still isn’t clear. My fiancée was with us in the restaurant, by the way. Erin thought it important that she be included in the discussion; Erin wanted her to know that she was “willing t...'\n",
            " 'Chances are you’ve heard some form of Parkinson’s law: “Work expands so as to fill up the time available for its completion.” You’ve also undoubtedly heard of Murphy’s law: “Whatever can go wrong will go wrong.” Well, next time you’re at a swanky cocktail party and you want to impress somebody, try dropping Manson’s law of avoidance on them: The more something threatens your identity, the more you will avoid it. That means the more something threatens to change how you view yourself, how successful/unsuccessful you believe yourself to be, how well you see yourself living up to your values, the more you will avoid ever getting around to doing it. There’s a certain comfort that comes with knowing how you fit in the world. Anything that shakes up that comfort—even if it could potentially make your life better—is inherently scary. Manson’s law applies to both good and bad things in life. Making a million dollars could threaten your identity just as much as losing all your money; becoming a famous ...'\n",
            " 'Buddhism argues that your idea of who “you” are is an arbitrary mental construction and that you should let go of the idea that “you” exist at all; that the arbitrary metrics by which you define yourself actually trap you, and thus you’re better off letting go of everything. In a sense, you could say that Buddhism encourages you to not give a fuck. It sounds wonky, but there are some psychological benefits to this approach to life. When we let go of the stories we tell about ourselves, to ourselves, we free ourselves up to actually act (and fail) and grow. When someone admits to herself, “You know, maybe I’m not good at relationships,” then she issuddenly free to act and end her bad marriage. She has no identity to protect by staying in a miserable, crappy marriage just to prove something to herself. When the student admits to himself, “You know, maybe I’m not a rebel; maybe I’m just scared,” then he’s free to be ambitious again. He has no reason to feel threatened by pursuing his academic dreams and mayb...'\n",
            " 'A friend of mine recently got engaged to be married. The guy who proposed to her is pretty solid. He doesn’t drink. He doesn’t hit her or mistreat her. He’s friendly and has a good job. But since the engagement, my friend’s brother has been admonishing her nonstop about her immature life choices, warning her that she’s going to hurt herself with this guy, that she’s making a mistake, that she’s being irresponsible. And whenever my friend asks her brother, “What is your problem? Why does this bother you so much?” he acts as though there is no problem, that nothingabout the engagement bothers him, that he’s just trying to be helpful and look out for his little sister. But it’s clear that something does bother him. Perhaps it’s his own insecurities about getting married. Perhaps it’s a sibling rivalry thing. Perhaps it’s jealousy. Perhaps he’s just so caught up in his own victimhood that he doesn’t know how to show happiness for others without trying to make them feel miserable first. As a gener...'\n",
            " 'lear that something does bother him. Perhaps it’s his own insecurities about getting married. Perhaps it’s a sibling rivalry thing. Perhaps it’s jealousy. Perhaps he’s just so caught up in his own victimhood that he doesn’t know how to show happiness for others without trying to make them feel miserable first. As a general rule, we’re all the world’s worst observers of ourselves. When we’re angry, or jealous, or upset, we’re oftentimes the last ones to figure it out. And the only way to figure it out is to put cracks in our armor of certainty by consistently questioning how wrong we might be about ourselves. “Am I jealous—and if I am, then why?” “Am I angry?” “Is she right, and I’m just protecting my ego?” Questions like these need to become a mental habit. In many cases, the simple act of asking ourselves such questions generates the humility and compassion needed to resolve a lot of our issues. But it’s important to note that just because you ask yourself if you have the wrong idea doesn’t necessarily me...'\n",
            " 'I really mean it when I say it: I was fortunate. I graduated college in 2007, just in time for the financial collapse and Great Recession, and attempted to enter the worst job market in more than eighty years. Around the same time, I found out that the person who was subletting one of the rooms in my apartment hadn’t paid any rent for three months. When confronted about this, she cried and then disappeared, leaving my other roommate and me to cover everything. Goodbye, savings. I spent the next six months living on a friend’s couch, stringing together odd jobs and trying to stay in as little debt as possible while looking for a “real job.” I say I was fortunate because I entered the adult world already a failure. I started out at rock bottom. That’s basically everybody’s biggest fear later on in life, when confronted with starting a new business or changing careers or quitting an awful job, and I got to experience it right out of the gates. Things could only get better. So yeah, lucky. When you’re sleepi...'\n",
            " 'When Pablo Picasso was an old man, he was sitting in a café in Spain, doodling on a used napkin. He was nonchalant about the whole thing, drawing whatever amused him in that moment—kind of the same way teenage boys draw penises on bathroom stalls—except this was Picasso, so his bathroom- stall penises were more like cubist/impressionist awesomeness laced on top of faint coffee stains. Anyway, some woman sitting near him was looking on in awe. After a few moments, Picasso finished his coffee and crumpled up the napkin to throw away as he left. The woman stopped him. “Wait,” she said. “Can I have that napkin you were just drawing on? I’ll pay you for it.” “Sure,” Picasso replied. “Twenty thousand dollars.” The woman’s head jolted back as if he had just flung a brick at her. “What? It took you like two minutes to draw that.” “No, ma’am,” Picasso said. “It took me over sixty years to draw this.” He stuffed the napkin in his pocket and walked out of the café. Improvement at anything is based on thousands of ti...'\n",
            " 'In the 1950s, a Polish psychologist named Kazimierz Dabrowski studied World War II survivors and how they’d coped with traumatic experiences in the war. This was Poland, so things had been pretty gruesome. These people had experienced or witnessed mass starvation, bombings that turned cities to rubble, the Holocaust, the torture of prisoners of war, and the rape and/or murder of family members, if not by the Nazis, then a few years later by the Soviets. As Dabrowski studied the survivors, he noticed something both surprising and amazing. A sizable percentage of them believed that the wartime experiences they’d suffered, although painful and indeed traumatic, had actually caused them to become better, more responsible, and yes, even happier people. Many described their lives before the war as if they’d been different people then: ungrateful for and unappreciative of their loved ones, lazy and consumed by petty problems, entitled to all they’d been given. After the war they felt more confident, more sure of...'\n",
            " 'In 2008, after holding down a day job for all of six weeks, I gave up on the whole job thing to pursue an online business. At the time, I had absolutely no clue what I was doing, but I figured if I was going to be broke and miserable, I might as well be while working on my own terms. And at that time, all I seemed to really care about was chasing girls. So fuck it, I decided to start a blog about my crazy dating life. That first morning that I woke up self-employed, terror quickly consumed me. I found myself sitting with my laptop and realized, for the first time, that I was entirely responsible for all of my own decisions, as well as the consequences of those decisions. I was responsible for teaching myself web design, Internet marketing, search engine optimization, and other esoteric topics. It was all on my shoulders now. And so I did what any twenty-four-year-old who’d just quit his job and had no idea what he was doing would do: I downloaded some computer games and avoided work like it was the Ebola v...'\n",
            " 'In 2009, I gathered up all my possessions, sold them or put them into storage, left my apartment, and set off for Latin America. By this time my little dating advice blog was getting some traffic and I was actually making a modest amount of money selling PDFs and courses online. I planned on spending much of the next few years living abroad, experiencing new cultures, and taking advantage of the lower cost of living in a number of developing countries in Asia and Latin America to build my business further. It was the digital nomad dream and as a twenty-five-year-old adventure-seeker, it was exactly what I wanted out of life. But as sexy and heroic as my plan sounded, not all of the values driving me to this nomadic lifestyle were healthy ones. Sure, I had some admirable values going on—a thirst to see the world, a curiosity for people and culture, some old-fashioned adventure-seeking. But there also existed a faint outline of shame underlying everything else. At the time I was hardly aware o...']\n"
          ]
        }
      ],
      "source": [
        "path_to_file = pathlib.Path(path_to_data_file)\n",
        "# path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "  lines = text.splitlines()\n",
        "  print(type(lines))\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "  pairs = pairs[:DATA_LIMIT]\n",
        "  # text = np.array(pairs)\n",
        "\n",
        "  text = np.array([target for target in pairs])\n",
        "\n",
        "  return text\n",
        "\n",
        "data = load_data(path_to_file)\n",
        "print(data.shape)\n",
        "\n",
        "data = data[:, 0]\n",
        "print(data.shape)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MnaJdnRSqPM",
        "outputId": "544ce993-32be-499a-e784-ba6f8fb33ce8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-TD1UFfRS7Kb"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(data)\n",
        "BATCH_SIZE = 8\n",
        "is_train = np.random.uniform(size=(len(data),)) < 0.9\n",
        "train_raw = tf.data.Dataset.from_tensor_slices(data[is_train]).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "val_raw = tf.data.Dataset.from_tensor_slices(data[~is_train] ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o62iyD_IGKWW",
        "outputId": "de36652b-6e5c-455d-bef5-a366eee14b83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ak5OdqfTrH_",
        "outputId": "9ce9f3e0-fc44-452c-eeae-4daab195ab14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'About twenty-five hundred years ago, in the Himalayan foothills of present-day Nepal, there lived in a great palace a king who was going to have a son. For this son the king had a particularly grand idea: he would make the child\\xe2\\x80\\x99s life perfect. The child would never know a moment of suffering\\xe2\\x80\\x94every need, every desire, would be accounted for at all times. The king built high walls around the palace that prevented the prince from knowing the outside world. He spoiled the child, lavishing him with food and gifts, surrounding him with servants who catered to his every whim. And just as planned, the child grew up ignorant of the routine cruelties of human existence. All of the prince\\xe2\\x80\\x99s childhood went on like this. But despite the endless luxury and opulence, the prince became kind of a pissed-off young man. Soon, every experience felt empty and valueless. The problem was that no matter what his father gave him, it never seemed enough, never meant anything. So late one night, the prince snuck out of the palace...'\n",
            " b'Years ago, when I was much younger and stupider, I wrote a blog post, and at the end of it I said something like, \\xe2\\x80\\x9cAnd as a great philosopher once said: \\xe2\\x80\\x98With great power comes great responsibility.\\xe2\\x80\\x99\\xe2\\x80\\x9d It sounded nice and authoritative. I couldn\\xe2\\x80\\x99t remember who had said it, and my Google search had turned up nothing, but I stuck it in there anyway. It fit the post nicely. About ten minutes later, the first comment came in: \\xe2\\x80\\x9cI think the \\xe2\\x80\\x98great philosopher\\xe2\\x80\\x99 you\\xe2\\x80\\x99re referring to is Uncle Ben from the movie Spider-Man.\\xe2\\x80\\x9d As another great philosopher once said, \\xe2\\x80\\x9cDoh!\\xe2\\x80\\x9d \\xe2\\x80\\x9cWith great power comes great responsibility.\\xe2\\x80\\x9d The last words of Uncle Ben before a thief whom Peter Parker let get away murders him on a sidewalk full of people for absolutely no explicable reason. That great philosopher. Still, we\\xe2\\x80\\x99ve all heard the quote. It gets repeated a lot\\xe2\\x80\\x94usually ironically and after about seven beers. It\\xe2\\x80\\x99s one of those perfect quotes that sound really intelligent, and yet it\\xe2\\x80\\x99s basically just telling...'\n",
            " b'There\\xe2\\x80\\x99s an insidious quirk to your brain that, if you let it, can drive you absolutely batty. Tell me if this sounds familiar to you: You get anxious about confronting somebody in your life. That anxiety cripples you and you start wondering why you\\xe2\\x80\\x99re so anxious. Now you\\xe2\\x80\\x99re becoming anxious about being anxious. Oh no! Doubly anxious! Now you\\xe2\\x80\\x99re anxious about your anxiety, which is causing more anxiety. Quick, where\\xe2\\x80\\x99s the whiskey? Or let\\xe2\\x80\\x99s say you have an anger problem. You get pissed off at the stupidest, most inane stuff, and you have no idea why. And the fact that you get pissed off so easily starts to piss you off even more. And then, in your petty rage, you realize that being angry all the time makes you a shallow and mean person, and you hate this; you hate it so much that you get angry at yourself. Now look at you: you\\xe2\\x80\\x99re angry at yourself getting angry about being angry. Fuck you, wall. Here, have a fist. Or you\\xe2\\x80\\x99re so worried about doing the right thing all the time that you become wor...'\n",
            " b'I really mean it when I say it: I was fortunate. I graduated college in 2007, just in time for the financial collapse and Great Recession, and attempted to enter the worst job market in more than eighty years. Around the same time, I found out that the person who was subletting one of the rooms in my apartment hadn\\xe2\\x80\\x99t paid any rent for three months. When confronted about this, she cried and then disappeared, leaving my other roommate and me to cover everything. Goodbye, savings. I spent the next six months living on a friend\\xe2\\x80\\x99s couch, stringing together odd jobs and trying to stay in as little debt as possible while looking for a \\xe2\\x80\\x9creal job.\\xe2\\x80\\x9d I say I was fortunate because I entered the adult world already a failure. I started out at rock bottom. That\\xe2\\x80\\x99s basically everybody\\xe2\\x80\\x99s biggest fear later on in life, when confronted with starting a new business or changing careers or quitting an awful job, and I got to experience it right out of the gates. Things could only get better. So yeah, lucky. When you\\xe2\\x80\\x99re sleepi...'\n",
            " b'Good values are 1) reality-based, 2) socially constructive, and 3) immediate and controllable. Bad values are 1) superstitious, 2) socially destructive, and 3) not immediate or controllable. Honesty is a good value because it\\xe2\\x80\\x99s something you have complete control over, it reflects reality, and it benefits others (even if it\\xe2\\x80\\x99s sometimes unpleasant). Popularity, on the other hand, is a bad value. If that\\xe2\\x80\\x99s your value, and if your metric is being the most popular guy/girl at the dance party, much of what happens will be out of your control: you don\\xe2\\x80\\x99t know who else will be at the event, and you probably won\\xe2\\x80\\x99t know who half those people are. Second, the value/metric isn\\xe2\\x80\\x99t based on reality: you may feel popular or unpopular, when in fact you have no fucking clue what anybody else really thinks about you. (Side Note: As a rule, people who are terrified of what others think about them are actually terrified of all the shitty things they think about themselves being reflected back at them.) Some examples of goo...'\n",
            " b'question, then, is, What do we give a fuck about? What are we choosing to give a fuck about? And how can we not give a fuck about what ultimately does not matter? My mother was recently screwed out of a large chunk of money by a close friend of hers. Had I been indifferent, I would have shrugged my shoulders, sipped my mocha, and downloaded another season of The Wire. Sorry, Mom. But instead, I was indignant. I was pissed off. I said, \\xe2\\x80\\x9cNo, screw that, Mom. We\\xe2\\x80\\x99re going to lawyer the fuck up and go after this asshole. Why? Because I don\\xe2\\x80\\x99t give a fuck. I will ruin this guy\\xe2\\x80\\x99s life if I have to.\\xe2\\x80\\x9d This illustrates the first subtlety of not giving a fuck. When we say, \\xe2\\x80\\x9cDamn, watch out, Mark Manson just don\\xe2\\x80\\x99t give a fuck,\\xe2\\x80\\x9d we don\\xe2\\x80\\x99t mean that Mark Manson doesn\\xe2\\x80\\x99t care about anything; on the contrary, we mean that Mark Manson doesn\\xe2\\x80\\x99t care about adversity in the face of his goals, he doesn\\xe2\\x80\\x99t care about pissing some people off to do what he feels is right or important or noble. We mean that Mark Manson is the type of ...'\n",
            " b'Try this. Take a random person and put them in a room with some buttons to push. Then tell them that if they do something specific\\xe2\\x80\\x94some undefined something that they have to figure out\\xe2\\x80\\x94a light will flash on indicating that they\\xe2\\x80\\x99ve won a point. Then tell them to see how many points they can earn within a thirty-minute period. When psychologists have done this, what happens is what you might expect. People sit down and start mashing buttons at random until eventually the light comes on to tell them they got a point. Logically, they then try repeating whatever they were doing to get more points. Except now the light\\xe2\\x80\\x99s not coming on. So they start experimenting with more complicated sequences\\xe2\\x80\\x94press this button three times, then this button once, then wait five seconds, and\\xe2\\x80\\x94ding! Another point. But eventually that stops working. Perhaps it doesn\\xe2\\x80\\x99t have to do with buttons at all, they think. Perhaps it has to do with how I\\xe2\\x80\\x99m sitting. Or what I\\xe2\\x80\\x99m touching. Maybe it has to do with my feet. Ding! Another point. Ye...'\n",
            " b'A friend of mine recently got engaged to be married. The guy who proposed to her is pretty solid. He doesn\\xe2\\x80\\x99t drink. He doesn\\xe2\\x80\\x99t hit her or mistreat her. He\\xe2\\x80\\x99s friendly and has a good job. But since the engagement, my friend\\xe2\\x80\\x99s brother has been admonishing her nonstop about her immature life choices, warning her that she\\xe2\\x80\\x99s going to hurt herself with this guy, that she\\xe2\\x80\\x99s making a mistake, that she\\xe2\\x80\\x99s being irresponsible. And whenever my friend asks her brother, \\xe2\\x80\\x9cWhat is your problem? Why does this bother you so much?\\xe2\\x80\\x9d he acts as though there is no problem, that nothingabout the engagement bothers him, that he\\xe2\\x80\\x99s just trying to be helpful and look out for his little sister. But it\\xe2\\x80\\x99s clear that something does bother him. Perhaps it\\xe2\\x80\\x99s his own insecurities about getting married. Perhaps it\\xe2\\x80\\x99s a sibling rivalry thing. Perhaps it\\xe2\\x80\\x99s jealousy. Perhaps he\\xe2\\x80\\x99s just so caught up in his own victimhood that he doesn\\xe2\\x80\\x99t know how to show happiness for others without trying to make them feel miserable first. As a gener...'], shape=(8,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for context_string in train_raw.take(1):\n",
        "  print(context_string[:])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEIb-8WzXBH_",
        "outputId": "c52c1605-3416-4bf0-cc86-1b5ceceeedf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'You have been invited to think of the two systems as agents within the mind.'\n",
            "b'You have been invited to think of the two systems as agents within the mind.'\n"
          ]
        }
      ],
      "source": [
        "example_text = tf.constant(\"You have been invited to think of the two systems as agents within the mind.\")\n",
        "print(example_text.numpy())\n",
        "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RDW7o5jjXziy"
      },
      "outputs": [],
      "source": [
        "# @tf.keras.utils.register_keras_serializable(package='Custom', name=None)\n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accented characters\n",
        "  # text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # add space arround punctuation\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # remove non-desplayable characters\n",
        "  text = tf.strings.regex_replace(text, '[^\\x00-\\x7F]+', '')\n",
        "  #Strip white space\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxC8leRiYUrP",
        "outputId": "bc8992fa-c258-473e-a6d2-dadfee40f731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have been invited to think of the two systems as agents within the mind.\n",
            "[START] you have been invited to think of the two systems as agents within the mind . [END]\n"
          ]
        }
      ],
      "source": [
        "print(example_text.numpy().decode())\n",
        "print(tf_lower_and_split_punct(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CEKR0cSGYXdp"
      },
      "outputs": [],
      "source": [
        "#Text Vectorization for the context text data (spainish)\n",
        "max_vocab_size = 30000\n",
        "context_text_processor = tf.keras.layers.TextVectorization(standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhe-V3qHbLFL",
        "outputId": "43fc33d0-28bb-4ecd-d4b1-a79567a17fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['', '[UNK]', '.', ',', 'the', 'and', 'to', 'a', 'of', 'you', 'that', 'i', 'in', 'was', 'it', 'on', 'his', 'he', 'for', 'with']\n"
          ]
        }
      ],
      "source": [
        "context_text_processor.adapt(train_raw)\n",
        "print(context_text_processor.get_vocabulary()[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pk.dump(context_text_processor.get_config(), open('text_processor_config.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhXyFmFabmrG",
        "outputId": "5a45031e-48b0-4f4b-df59-fa0920b761ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<tf.RaggedTensor [[26, 22, 737, 540, 97, 251, 3, 12, 4, 1318, 1397, 8, 1049, 1143, 3, 68,\n",
            "  519, 12, 7, 79, 296, 7, 320, 60, 13, 80, 6, 39, 7, 434, 2, 18, 28, 434, 4,\n",
            "  320, 32, 7, 294, 1351, 137, 17, 55, 101, 4, 1637, 42, 216, 2, 4, 197, 55,\n",
            "  133, 85, 7, 504, 8, 818, 219, 3, 117, 1533, 3, 55, 37, 1796, 18, 23, 34,\n",
            "  265, 2, 4, 320, 1666, 543, 697, 171, 4, 296, 10, 1047, 4, 290, 104, 525,\n",
            "  4, 297, 150, 2, 17, 865, 4, 197, 3, 1242, 43, 19, 341, 5, 1364, 3, 806,\n",
            "  43, 19, 927, 60, 1652, 6, 16, 117, 678, 2, 5, 58, 21, 482, 3, 4, 197, 552,\n",
            "  47, 1300, 8, 4, 959, 1570, 8, 326, 1445, 2, 34, 8, 4, 1046, 1639, 383, 15,\n",
            "  50, 28, 2, 29, 595, 4, 1468, 1203, 5, 1108, 3, 4, 290, 1709, 229, 8, 7,\n",
            "  1070, 376, 311, 2, 433, 3, 117, 344, 191, 582, 5, 710, 2, 4, 130, 13, 10,\n",
            "  57, 309, 24, 16, 116, 335, 43, 3, 14, 133, 283, 346, 3, 133, 1186, 144, 2,\n",
            "  38, 1245, 91, 184, 3, 4, 290, 885, 49, 8, 4, 296, 2, 2, 2, 27]            ,\n",
            " [26, 97, 251, 3, 40, 11, 13, 92, 375, 5, 831, 3, 11, 654, 7, 361, 291, 3,\n",
            "  5, 23, 4, 347, 8, 14, 11, 99, 74, 50, 3, 5, 21, 7, 79, 215, 217, 99, 19,\n",
            "  79, 478, 167, 79, 113, 2, 14, 432, 302, 5, 1732, 2, 11, 603, 989, 60, 32,\n",
            "  99, 14, 3, 5, 25, 1354, 453, 32, 263, 47, 497, 3, 29, 11, 274, 14, 12, 68,\n",
            "  636, 2, 14, 561, 4, 291, 1139, 2, 22, 786, 221, 228, 3, 4, 105, 1614, 248,\n",
            "  12, 11, 155, 4, 79, 215, 67, 998, 6, 20, 398, 624, 104, 4, 1156, 868, 2,\n",
            "  21, 202, 79, 215, 217, 99, 3, 1511, 173, 19, 79, 478, 167, 79, 113, 2, 4,\n",
            "  319, 664, 8, 398, 624, 200, 7, 777, 675, 1078, 1093, 226, 63, 368, 1152,\n",
            "  43, 15, 7, 903, 1385, 8, 48, 18, 374, 57, 1438, 465, 2, 10, 79, 215, 2,\n",
            "  427, 3, 682, 34, 329, 4, 1018, 2, 14, 334, 986, 7, 1209, 1273, 5, 123, 22,\n",
            "  925, 1708, 2, 45, 91, 8, 268, 216, 1017, 10, 276, 75, 1280, 3, 5, 253, 45,\n",
            "  627, 58, 789, 2, 2, 2, 27]                                                ,\n",
            " [26, 271, 64, 1281, 1022, 6, 33, 1674, 10, 3, 36, 9, 226, 14, 3, 81, 1494,\n",
            "  9, 374, 1712, 2, 98, 65, 36, 28, 431, 1428, 6, 9, 9, 63, 145, 22, 1602,\n",
            "  435, 12, 33, 42, 2, 10, 201, 1574, 9, 5, 9, 128, 377, 111, 67, 38, 145, 2,\n",
            "  115, 67, 626, 145, 22, 119, 145, 2, 495, 57, 173, 1501, 145, 173, 115, 67,\n",
            "  145, 22, 33, 201, 3, 257, 20, 612, 53, 201, 2, 1023, 3, 680, 4, 677, 30,\n",
            "  35, 318, 83, 9, 39, 64, 1761, 130, 2, 9, 63, 180, 78, 23, 4, 830, 3, 71,\n",
            "  1292, 834, 3, 5, 9, 39, 57, 137, 111, 2, 5, 4, 164, 10, 9, 63, 180, 78,\n",
            "  38, 1483, 853, 6, 1071, 9, 78, 165, 53, 2, 5, 52, 3, 12, 33, 483, 1014, 3,\n",
            "  9, 1006, 10, 119, 146, 34, 4, 44, 312, 9, 7, 918, 5, 161, 182, 3, 5, 9,\n",
            "  330, 28, 9, 330, 14, 38, 92, 10, 9, 63, 146, 23, 82, 2, 115, 163, 23, 9,\n",
            "  67, 146, 23, 82, 140, 146, 22, 119, 146, 2, 72, 9, 3, 698, 2, 233, 3, 39,\n",
            "  7, 1409, 2, 35, 67, 38, 657, 22, 166, 4, 129, 126, 34, 4, 44, 10, 9, 121,\n",
            "  665, 2, 2, 2, 27]                                                         ]>\n"
          ]
        }
      ],
      "source": [
        "# now layers can convert batch of strings to batch of token ids\n",
        "example_tokens = context_text_processor(context_string)\n",
        "print(example_tokens[:3, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzyL30lHcKHj",
        "outputId": "17107639-a6bf-4194-82c9-7b6421da6083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[START] about twentyfive hundred years ago , in the himalayan foothills of presentday nepal , there lived in a great palace a king who was going to have a son . for this son the king had a particularly grand idea he would make the childs life perfect . the child would never know a moment of sufferingevery need , every desire , would be accounted for at all times . the king built high walls around the palace that prevented the prince from knowing the outside world . he spoiled the child , lavishing him with food and gifts , surrounding him with servants who catered to his every whim . and just as planned , the child grew up ignorant of the routine cruelties of human existence . all of the princes childhood went on like this . but despite the endless luxury and opulence , the prince became kind of a pissedoff young man . soon , every experience felt empty and valueless . the problem was that no matter what his father gave him , it never seemed enough , never meant anything . so late one night , the prince snuck out of the palace . . . [END]\n"
          ]
        }
      ],
      "source": [
        "#The get_vocabulary method can be used to convert token IDs back to text:\n",
        "context_vocab = np.array(context_text_processor.get_vocabulary())\n",
        "tokens = context_vocab[example_tokens[0].numpy()]\n",
        "tokens = ' '.join(tokens)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bj7hohYnckrp",
        "outputId": "bec739e3-8760-4423-bb77-2df06127a872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "process_text\n",
            "process_text\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def process_text(context):\n",
        "  target = context_text_processor(context)\n",
        "  context = context_text_processor(context).to_tensor()\n",
        "  # print(type(context))\n",
        "  # targ_in = target[:, :-1].to_tensor() #take everthing in axiz = 0 and take everything except the last in axis = 2\n",
        "  # targ_in = target[:, 1:].to_tensor()\n",
        "  # targ_out = target[:, :-1].to_tensor()\n",
        "  target = target[:, 1:]\n",
        "  targ_in = target[:, :-1].to_tensor() #take everthing in axiz = 0 and take everything except the last in axis = 2\n",
        "  targ_out = target[:, 1:].to_tensor()\n",
        "\n",
        "  \n",
        "\n",
        "  print('process_text')\n",
        "\n",
        "  return (context, targ_in), targ_out\n",
        "\n",
        "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_ds = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnWTGDyDEjBr",
        "outputId": "bd240788-a004-462d-b8a1-0e094a76c5ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WJihLzsdCao",
        "outputId": "2c93be65-d814-4553-d589-1af8ceba6adb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 26  36  11 170   9   3  24  54   9  89] (225,)\n",
            "[ 36  11 170   9   3  24  54   9  89  49] (223,)\n",
            "[ 11 170   9   3  24  54   9  89  49   8] (223,)\n",
            "\n",
            "[ 26  36  11 142 530   7 815   3  11  55] (215,)\n",
            "[ 36  11 142 530   7 815   3  11  55 530] (213,)\n",
            "[ 11 142 530   7 815   3  11  55 530  91] (213,)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for (example_context_tokens, target_in_tokens), target_out_tokens in train_ds.take(2):\n",
        "\n",
        "  print(example_context_tokens[0, :10].numpy(), example_context_tokens[0, :].numpy().shape)\n",
        "  print(target_in_tokens[0, :10].numpy(), target_in_tokens[0, :].numpy().shape)\n",
        "  print(target_out_tokens[0, :10].numpy(), target_out_tokens[0, :].numpy().shape)\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "F4lUixvWV0eL"
      },
      "outputs": [],
      "source": [
        "# target_in_tokens = target_in_tokens + 1\n",
        "# print(target_in_tokens[0, :10].numpy(),'\\n', target_out_tokens[0, :10].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gccz0Pib9hcj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H8BHQKIedKKL"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The encoder:\n",
        "  1. Takes a list of token IDs (from context_text_processor).\n",
        "  2. Looks up an embedding vector for each token (Using a layers.Embedding).\n",
        "  3. Processes the embeddings into a new sequence (Using a bidirectional layers.GRU).\n",
        "  4. Returns the processed sequence. This will be passed to the attention head.\n",
        "'''\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.units = units\n",
        "\n",
        "    #The embedding layer converts tokens into vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.units, mask_zero=True)\n",
        "\n",
        "    #The RNN layer processes those vectors sequentially\n",
        "    self.rnn = tf.keras.layers.Bidirectional(merge_mode='sum', \n",
        "                                             layer=tf.keras.layers.GRU(self.units, return_sequences=True, recurrent_initializer='glorot_uniform' ))\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = self.embedding(x)\n",
        "    x = self.rnn(x)\n",
        "    # print('call')\n",
        "    return x\n",
        "\n",
        "  def convert_input(self, texts):\n",
        "    texts = tf.convert_to_tensor(texts)\n",
        "    if len(texts.shape) == 0:\n",
        "      texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
        "    context = self.text_processor(texts).to_tensor()\n",
        "    context = self(context)\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VLCMOwS5dZ5C"
      },
      "outputs": [],
      "source": [
        "UNITS = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shfWDGBPdb58",
        "outputId": "208acf52-57f4-4079-92fc-1ada0c870c47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context tokens, shape (batch, s): (8, 215)\n",
            "Encoder output, shape (batch, s, units): (8, 215, 256)\n"
          ]
        }
      ],
      "source": [
        "# Encode the input sequence.\n",
        "encoder = Encoder(context_text_processor, UNITS)\n",
        "ex_context = encoder(example_context_tokens)\n",
        "\n",
        "print(f'Context tokens, shape (batch, s): {example_context_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {ex_context.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Z7x-Gtg8dd0g"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=1, **kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, x, context):\n",
        "    atten_output, atten_score = self.mha(query=x, value=context, return_attention_scores=True)\n",
        "    x = self.add([x, atten_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mET4qJxxdyMG"
      },
      "outputs": [],
      "source": [
        "attention_layer = CrossAttention(UNITS)\n",
        "# Attend to the encoded tokens\n",
        "embed = tf.keras.layers.Embedding(context_text_processor.vocabulary_size(),\n",
        "                                  output_dim=UNITS, mask_zero=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8vNl2zVLd0Wq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The decoder's job is to generate predictions for the next token at each location in the target sequence.\n",
        "  1. It looks up embeddings for each token in the target sequence.\n",
        "  2. It uses an RNN to process the target sequence, and keep track of what it has generated so far.\n",
        "  3. It uses RNN output as the \"query\" to the attention layer, when attending to the encoder's output.\n",
        "  4. At each location in the output it predicts the next token.\n",
        "'''\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, text_processor, units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.text_processor = text_processor\n",
        "    self.vocab_size = text_processor.vocabulary_size()\n",
        "    self.word_to_id = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]')\n",
        "    self.id_to_word = tf.keras.layers.StringLookup(vocabulary=text_processor.get_vocabulary(), mask_token='', oov_token='[UNK]', invert=True)\n",
        "    self.start_token = self.word_to_id('[START]')\n",
        "    self.end_token = self.word_to_id('[END]')\n",
        "\n",
        "    self.units = units\n",
        "\n",
        "    # 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.vocab_size, self.units, mask_zero=True)\n",
        "    # 2. The RNN keeps track of what's been generated so far.\n",
        "    self.rnn = tf.keras.layers.GRU(self.units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "    #3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = CrossAttention(self.units)\n",
        "\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units, activation='relu')\n",
        "    # 4. This fully connected layer produces the logits for each output token.\n",
        "    self.output_layer = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "  def call(self, context, x, state=None, return_state=False):\n",
        "    #Lookup for embeddings\n",
        "    x = self.embedding(x)\n",
        "    #Process the target sequence\n",
        "    x, state = self.rnn(x, initial_state=state)\n",
        "    #Use the rnn output as the query for the attention over the context\n",
        "    x = self.attention(x, context)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    #generate logit predictyions for the next token\n",
        "    logits = self.output_layer(x)\n",
        "\n",
        "\n",
        "    if return_state:\n",
        "      return logits, state\n",
        "    return logits\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JGJhPEvRd-eC"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(context_text_processor, UNITS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Aj2NEDLZeBd-"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_initial_state(self, context):\n",
        "  batch_size = tf.shape(context)[0]\n",
        "  print(batch_size)\n",
        "  start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  # print(start_tokens, 'satrt token')\n",
        "  # print(tf.constant(context), 'context')\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "  embedded = self.embedding(start_tokens)\n",
        "  return start_tokens, done, self.rnn.get_initial_state(embedded)[0]\n",
        "\n",
        "# print(example_context_tokens, 'ex_context_tokens')\n",
        "# ex_context = encoder(example_context_tokens)\n",
        "# next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "# print(next_token, state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mBmT2Ia2eHjd"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def tokens_to_text(self, tokens):\n",
        "  words = self.id_to_word(tokens)\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  result = tf.strings.regex_replace(result, '^ *\\[START\\] *', '')\n",
        "  result = tf.strings.regex_replace(result, ' *\\[END\\] *$', '')\n",
        "  return result\n",
        "\n",
        "# decoder.tokens_to_text(example_context_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2jwSPALseI8x"
      },
      "outputs": [],
      "source": [
        "@Decoder.add_method\n",
        "def get_next_token(self, context, next_token, done, state, temperature = 0.0):\n",
        "\n",
        "  logits, state = self(context, next_token, state = state, return_state=True) \n",
        "\n",
        "  if temperature == 0.0:\n",
        "    next_token = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = logits[:, -1, :]/temperature\n",
        "    next_token = tf.random.categorical(logits, num_samples=1)\n",
        "\n",
        "  # If a sequence produces an `end_token`, set it `done`\n",
        "  # done = done | (next_token == self.end_token)\n",
        "  # # Once a sequence is done it only produces 0-padding.\n",
        "  # next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
        "\n",
        "  return next_token, done, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyF377dDeKge",
        "outputId": "78cc582a-479d-4a40-e88a-2d8f6327d608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(8, shape=(), dtype=int32)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([b'catch experimenting wednesday point faint author morning gotten flung age',\n",
              "       b'batty membership bleeding ability quite visited rape regularly wait humdrum',\n",
              "       b'rejecting fill young alcoholic wonky , hell immediate place studied'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup the loop variables.\n",
        "next_token, done, state = decoder.get_initial_state(ex_context)\n",
        "tokens = []\n",
        "\n",
        "for n in range(10):\n",
        "  # Run one step.\n",
        "  next_token, done, state = decoder.get_next_token(\n",
        "      ex_context, next_token, done, state, temperature=1.0)\n",
        "  # Add the token to the output.\n",
        "  tokens.append(next_token)\n",
        "\n",
        "# Stack all the tokens together.\n",
        "tokens = tf.concat(tokens, axis=-1) # (batch, t)\n",
        "\n",
        "# Convert the tokens back to a a string\n",
        "result = decoder.tokens_to_text(tokens)\n",
        "result[:3].numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RBokFp4beNDI"
      },
      "outputs": [],
      "source": [
        "class TextGen(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, units, context_text_processor, target_text_processor):\n",
        "    super().__init__()\n",
        "    #Build the encoder and the decoder\n",
        "    self.encoder = Encoder(context_text_processor, units)\n",
        "    self.decoder = Decoder(context_text_processor, units)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # print(\"here1\")\n",
        "    context, x = inputs\n",
        "    # print(\"here2\")\n",
        "\n",
        "    context = self.encoder(context)\n",
        "    # print(\"here3\")\n",
        "\n",
        "    logits = self.decoder(context, x)\n",
        "    # print(\"here4\")\n",
        "\n",
        "    try:\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "    # print(\"here5\", logits)\n",
        "    return logits\n",
        "  \n",
        "  def gen(self,\n",
        "                texts, *,\n",
        "                max_length=50,\n",
        "                temperature=0.0):\n",
        "    # Process the input texts\n",
        "    context = self.encoder.convert_input(texts)\n",
        "    batch_size = tf.shape(texts)[0]\n",
        "\n",
        "    # Setup the loop inputs\n",
        "    tokens = []\n",
        "    attention_weights = []\n",
        "    next_token, done, state = self.decoder.get_initial_state(context)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      # Generate the next token\n",
        "      next_token, done, state = self.decoder.get_next_token(context, next_token, done,  state, temperature)\n",
        "\n",
        "      # Collect the generated tokens\n",
        "      tokens.append(next_token)\n",
        "      # attention_weights.append(self.decoder.last_attention_weights)\n",
        "\n",
        "      # if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      #   break\n",
        "\n",
        "    # Stack the lists of tokens and attention weights.\n",
        "    tokens = tf.concat(tokens, axis=-1)   # t*[(batch 1)] -> (batch, t)\n",
        "    # self.last_attention_weights = tf.concat(attention_weights, axis=1)  # t*[(batch 1 s)] -> (batch, t s)\n",
        "\n",
        "    result = self.decoder.tokens_to_text(tokens)\n",
        "    return result\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dbeaZcS3edil"
      },
      "outputs": [],
      "source": [
        "model = TextGen(UNITS, context_text_processor, context_text_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Y4Ea88nwehHJ"
      },
      "outputs": [],
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "    # print(y_true.shape, y_pred.shape, \"masked_loss\")\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "    \n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "kWY7CEP5ejoz"
      },
      "outputs": [],
      "source": [
        "#custom accuracy\n",
        "def masked_acc(y_true, y_pred):\n",
        "  # Calculate the loss for each item in the batch\n",
        "  y_pred = tf.argmax(y_pred, axis=-1)\n",
        "  y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "\n",
        "  match = tf.cast(y_true == y_pred, dtype=tf.float32)\n",
        "  mask = tf.cast(y_true != 0, tf.float32)\n",
        "\n",
        "  return tf.reduce_sum(match) / tf.reduce_sum(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "X2NwmkorelRF"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=masked_loss, metrics=[masked_acc, masked_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPBGhwpWenSV",
        "outputId": "dc0f453f-82a2-4a7e-f6c6-047dbfba0dbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'expected_loss': 7.4977617, 'expected_acc': 0.0005543237250554324}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_size = 1.0 * context_text_processor.vocabulary_size()\n",
        "\n",
        "{\"expected_loss\": tf.math.log(vocab_size).numpy(),\n",
        " \"expected_acc\": 1/vocab_size}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tTowsTJZepLE"
      },
      "outputs": [],
      "source": [
        "# model.evaluate(val_ds, steps=20, return_dict=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_path = model_root_path + f\"/{DATA_LIMIT}_cp_{model_name}.ckpt\"\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFjl0XSdetaj",
        "outputId": "4d199a78-69fa-417b-8791-3c5bbaa06517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - ETA: 0s - loss: 6.3695 - masked_acc: 0.0640 - masked_loss: 6.3264WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 20 batches). You may need to use the repeat() function when building your dataset.\n",
            "20/20 [==============================] - 48s 2s/step - loss: 6.3695 - masked_acc: 0.0640 - masked_loss: 6.3264 - val_loss: 6.3365 - val_masked_acc: 0.0532 - val_masked_loss: 6.3365\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 25s 1s/step - loss: 5.5661 - masked_acc: 0.1394 - masked_loss: 5.5444\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 24s 1s/step - loss: 4.4652 - masked_acc: 0.2332 - masked_loss: 4.4551\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 23s 1s/step - loss: 3.1569 - masked_acc: 0.3695 - masked_loss: 3.1568\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 22s 1s/step - loss: 1.7840 - masked_acc: 0.6292 - masked_loss: 1.7928\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.8442 - masked_acc: 0.8528 - masked_loss: 0.8491\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.3702 - masked_acc: 0.9525 - masked_loss: 0.3720\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.1755 - masked_acc: 0.9840 - masked_loss: 0.1771\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 23s 1s/step - loss: 0.0841 - masked_acc: 0.9959 - masked_loss: 0.0837\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 22s 1s/step - loss: 0.0428 - masked_acc: 0.9990 - masked_loss: 0.0426\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(), \n",
        "    epochs=10,\n",
        "    steps_per_epoch = 20,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps = 20,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, monitor='masked_acc')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ouSULgTjSGAl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(1, shape=(), dtype=int32)\n",
            "i was doing , but i figured if i was going to be broke and miserable , i might as well be while working on my own terms . and at that time , all i seemed to really care about was chasing girls . so fuck it , i decided to start a blog about my crazy dating life . that first morning that i woke up selfemployed , terror quickly consumed me . i found myself sitting with my laptop and realized , for the first time , that i was entirely responsible for all of my own\n"
          ]
        }
      ],
      "source": [
        "result = model.gen(['This was one of my favorite'], max_length=100)\n",
        "result = result[0].numpy().decode()\n",
        "print(result)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAVE "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_path = f'../models/attention/{DATA_LIMIT}_model.tf'\n",
        "# vectorizer_config_path = '../models/attention/text_processor_config.pkl'\n",
        "# vectorizer_weights_path = '../models/attention/text_processor_weights.pkl'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "sP2ZseJ_SKmG"
      },
      "outputs": [],
      "source": [
        "# #save the vectorizaation layer\n",
        "# pickle.dump(context_text_processor.get_config(), open(vectorizer_config_path, 'wb'))\n",
        "# weights = context_text_processor.get_weights()\n",
        "# text_processor_weights = pickle.dump(weights, open(vectorizer_weights_path, \"wb\"))\n",
        "# #save the final fully trained  model\n",
        "# model.save(model_path)         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# loaded_model = None\n",
        "# loaded_model = tf.keras.models.load_model(model_path, custom_objects={'tf_lower_and_split_punct': tf_lower_and_split_punct}, compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result = loaded_model.predict(['This was one of my favorite'])\n",
        "# result = result[0].numpy().decode()\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# weights = context_text_processor.get_weights()\n",
        "\n",
        "# text_processor_weights = pickle.dump(weights, open(\"text_processor_weights.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "# loaded_config = pickle.load(open(\"text_processor_config.pkl\", 'rb'))\n",
        "# loaded_weights = pickle.load(open(\"text_processor_weights.pkl\", 'rb'))\n",
        "# text_processor = tf.keras.layers.TextVectorization.from_config(loaded_config)\n",
        "# text_processor.set_weights(loaded_weights)\n",
        "\n",
        "# new_model = TextGen(UNITS, text_processor, text_processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# new_model.load_weights(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# result = new_model.gen(['Hello world'])\n",
        "# result = result[0].numpy().decode()\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def create_model(path_to_model, path_to_vectorizer_config, path_to_vectorizer_weights):\n",
        "#     loaded_config = pickle.load(open(path_to_vectorizer_config, 'rb'))\n",
        "#     loaded_weights = pickle.load(open(path_to_vectorizer_weights, 'rb'))\n",
        "\n",
        "#     text_processor = tf.keras.layers.TextVectorization.from_config(loaded_config)\n",
        "#     text_processor.set_weights(loaded_weights)\n",
        "\n",
        "#     new_model = TextGen(UNITS, text_processor, text_processor)\n",
        "#     new_model.load_weights(path_to_model)\n",
        "\n",
        "#     return new_model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# textGenModel = create_model(path_to_model=model_path, path_to_vectorizer_config='text_processor_config.pkl', path_to_vectorizer_weights='text_processor_weights.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# textGenModel.gen(['This was one of my favorite and'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor(\"strided_slice_1:0\", shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "class Export(tf.Module):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "    def generate(self, inputs):\n",
        "        return self.model.gen(texts=inputs, max_length=500)\n",
        "\n",
        "export = Export(model)\n",
        "# _ = export.generate(tf.constant([\"Hello World\"]))\n",
        "tf.saved_model.save(export, model_path, signatures={'serving_defualt': export.generate})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(model_path)\n",
        "\n",
        "_ = reloaded.generate(tf.constant(['Hello World']))  #warmup\n",
        "result = reloaded.generate(tf.constant(['Blog post']))\n",
        "result[0].numpy().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = reloaded.generate(tf.constant(['Fuck']), max_length=100)\n",
        "result[0].numpy().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cccam",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "e572f4ff5b58607b77d6cc00cc4af0148511d0857c721aacfe8916b4ad8441a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
