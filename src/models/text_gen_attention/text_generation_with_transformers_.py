# -*- coding: utf-8 -*-
"""Text generation with Transformers  .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sAUnCAWLLJ-9Qf1XQFhtHKSGAE0c5mC1
"""

# Install the most re version of TensorFlow to use the improved
# masking support for `tf.keras.layers.MultiHeadAttention`.
# !apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2
# !pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text
# !pip install protobuf~=3.20.3
# !pip install -q tensorflow_datasets
# !pip install -q -U tensorflow-text tensorflow

# !pip install "tensorflow-text"

import logging
import time

import numpy as np
# import matplotlib.pyplot as plt

# import tensorflow_datasets as tfds
import tensorflow as tf

# import tensorflow_text
import pathlib

#Downloading dataset
# examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',with_info=True, as_supervised=True)

# train_examples, val_examples = examples['train'], examples['validation']

# for pt_examples, en_examples in train_examples.batch(3).take(1):
#   print('> Examples in Portuguese:')
#   for pt in pt_examples.numpy():
#     print(pt.decode('utf-8'))
#   print()

#   print('> Examples in English:')
#   for en in en_examples.numpy():
#     print(en.decode('utf-8'))

# convert csv to txt
def csv_to_txt( path, column, out_path):
  import pandas as pd
  data = pd.read_csv(path)
  data.dropna()
  print(data.head())
  column_contents = data[column].astype(str).values.tolist()
  column_contents
  # Write the column contents to a text file
  with open(out_path, 'w', encoding='utf-8') as f:
    for item in column_contents:
        item = item.encode('utf-8').decode('unicode_escape')
        f.write("%s\n" % item)
# csv_to_txt('/content/product description.csv', 'about_product')

DATA_LIMIT = 40
model_root_path = ''
model_path = f'{model_root_path}{DATA_LIMIT}_model.tf'
vectorizer_config_path = f'{model_root_path}text_processor_config.pkl'
vectorizer_weights_path = f'{model_root_path}text_processor_weights.pkl'
path_to_data_file = 'notebooks/bookstxt.'

path_to_file = pathlib.Path(path_to_data_file)
# path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'
def load_data(path):
  text = path.read_text(encoding='utf-8')
  lines = text.splitlines()
  print(type(lines))
  pairs = [line.split('\t') for line in lines]
  pairs = pairs[:DATA_LIMIT]
  # text = np.array(pairs)

  text = np.array([target for target in pairs])

  return text

data = load_data(path_to_file)
print(data.shape)

data = data[:, 0]
print(data.shape)
print(data)



model_name = 'ted_hrlr_translate_pt_en_converter'
tf.keras.utils.get_file(
    f'{model_name}.zip',
    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',
    cache_dir='.', cache_subdir='', extract=True
)

tokenizers = tf.saved_model.load(model_name)

[item for item in dir(tokenizers.en) if not item.startswith('_')]

BUFFER_SIZE = len(data)
BATCH_SIZE = 16
is_train = np.random.uniform(size=(len(data),)) < 0.95
train_raw = tf.data.Dataset.from_tensor_slices(data[is_train])
val_raw = tf.data.Dataset.from_tensor_slices(data[~is_train] )

# for example_string in train_raw.batch(1).take(1):
#   print(example_string)
#   break

# print('> This is a batch of strings:')
# for en in example.numpy():
#   print(en.decode('utf-8'))

# encoded = tokenizers.en.tokenize(en_examples)

# print('> This is a padded-batch of token IDs:')
# for row in encoded.to_list():
#   print(row)

# round_trip = tokenizers.en.detokenize(encoded)

# print('> This is human-readable text:')
# for line in round_trip.numpy():
#   print(line.decode('utf-8'))

# print('> This is the text split into tokens:')
# tokens = tokenizers.en.lookup(encoded)
# tokens

# lengths = []

# for pt_examples, en_examples in train_examples.batch(1024):
#   pt_tokens = tokenizers.pt.tokenize(pt_examples)
#   lengths.append(pt_tokens.row_lengths())

#   en_tokens = tokenizers.en.tokenize(en_examples)
#   lengths.append(en_tokens.row_lengths())
#   print('.', end='', flush=True)

# all_lengths = np.concatenate(lengths)

# plt.hist(all_lengths, np.linspace(0, 500, 101))
# plt.ylim(plt.ylim())
# max_length = max(all_lengths)
# plt.plot([max_length, max_length], plt.ylim())
# plt.title(f'Maximum tokens per example: {max_length}');

'''
Set up a data pipeline with tf.data
The following function takes batches of text as input, and converts them to a format suitable for training.

  It tokenizes them into ragged batches.
  It trims each to be no longer than MAX_TOKENS.
  It splits the target (English) tokens into inputs and labels. These are shifted by one step so that at each input location the label is the id of the next token.
  It converts the RaggedTensors to padded dense Tensors.
  It returns an (inputs, labels) pair.
'''

MAX_TOKENS=128
def prepare_batch(pt, en):
    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.
    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.
    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor

    en = tokenizers.en.tokenize(en)
    en = en[:, :(MAX_TOKENS+1)]
    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens
    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens

    return (pt, en_inputs), en_labels



def tf_lower_and_split_punct(text):
  # Split accented characters
  # text = tf_text.normalize_utf8(text, 'NFKD')
  text = tf.strings.lower(text)
  # Keep space, a to z, and select punctuation.
  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')
  # add space arround punctuation
  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \0 ')
  # remove non-desplayable characters
  text = tf.strings.regex_replace(text, '[^\x00-\x7F]+', '')
  #Strip white space
  text = tf.strings.strip(text)

  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')
  return text


def process_text(context):
  target = tokenizers.en.tokenize(context)
  context = tokenizers.en.tokenize(context).to_tensor()

  target = target[:, 1:]
  targ_in = target[:, :-1].to_tensor() #take everthing in axiz = 0 and take everything except the last in axis = 2
  targ_out = target[:, 1:].to_tensor()

  

  print('process_text')

  return (context, targ_in), targ_in

BUFFER_SIZE = 20000
BATCH_SIZE = 64

def make_batches(ds):
  return (
      ds
      .shuffle(BUFFER_SIZE)
      .batch(BATCH_SIZE)
      # .map(tf_lower_and_split_punct, tf.data.AUTOTUNE)
      .map(process_text, tf.data.AUTOTUNE)
      .prefetch(buffer_size=tf.data.AUTOTUNE))

# Create training and validation set batches.
train_batches = make_batches(train_raw)
val_batches = make_batches(val_raw)

for (context, targ_in), targ_out in train_batches.take(1):
  break

print(context.shape)
print(targ_in.shape)
print(targ_out.shape)

print(targ_in[0][:10])
print(targ_out[0][:10])

#The embedding and positional encoding layer
'''
A Transformer adds a "Positional Encoding" to the embedding vectors. 
It uses a set of sines and cosines at different frequencies (across the sequence). 
By definition nearby elements will have similar position encodings.
'''
def positional_encoding(length, depth):
  depth = depth/2

  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)
  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)

  angle_rates = 1 / (10000**depths)         # (1, depth)
  angle_rads = positions * angle_rates      # (pos, depth)

  pos_encoding = np.concatenate(
      [np.sin(angle_rads), np.cos(angle_rads)],
      axis=-1) 

  return tf.cast(pos_encoding, dtype=tf.float32)

class PositionalEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, d_model):
    super().__init__()
    self.d_model = d_model
    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) 
    self.pos_encoding = positional_encoding(length=2048, depth=d_model)

  def compute_mask(self, *args, **kwargs):
    return self.embedding.compute_mask(*args, **kwargs)

  def call(self, x):
    length = tf.shape(x)[1]
    x = self.embedding(x)
    # This factor sets the relative scale of the embedding and positonal_encoding.
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x = x + self.pos_encoding[tf.newaxis, :length, :]
    return x

# embed_context = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)
# embed_targ = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)

# context_emb = embed_context(context)
# targ_emb = embed_targ(targ_in)

'''
In Keras, masking is used to indicate which elements of an input sequence should be ignored by a layer. 
Masking is typically used when the input sequences have variable lengths and padding is used to make them 
all the same length. The masking process allows the model to ignore the padding tokens and only consider 
the meaningful tokens in the input sequence.
'''
# targ_emb._keras_mask

#Add and normalize
'''
The residual "Add & Norm" blocks are included so that training is efficient. 
The residual connection provides a direct path for the gradient 
(and ensures that vectors are updated by the attention layers instead of replaced), 
while the normalization maintains a reasonable scale for the outputs.
'''

#The base attention layer
'''
Attention layers are used throughout the model. 
These are all identical except for how the attention is configured. 
Each one contains a layers.MultiHeadAttention, a layers.LayerNormalization and a layers.Add.
'''
class BaseAttention(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()

'''
  The querys is what you're trying to find.
  The keys what sort of information the dictionary has.
  The value is that information.
'''

#The cross attention layer
#The output length is the length of the query sequence, and not the length of the context key/value sequence.

class CrossAttention(BaseAttention):
  def call(self, x, context):
    attn_output, attn_scores = self.mha( query=x, key=context, value=context, return_attention_scores=True)

    # Cache the attention scores for plotting later.
    self.last_attn_scores = attn_scores

    x = self.add([x, attn_output])
    x = self.layernorm(x)

    return x

# sample_ca = CrossAttention(num_heads=2, key_dim=512)

# print(context_emb.shape)
# print(targ_emb.shape)
# print(sample_ca(targ_emb, context_emb).shape)

#The global self attention layer
'''
This layer is responsible for processing the context sequence, and propagating information along its length:
(Since the context sequence is fixed while the translation is being generated, information is allowed to flow in both directions.)
The global self attention layer on the other hand lets every sequence element directly access every other sequence element, 
  with only a few operations, and all the outputs can be computed in parallel.
'''
class GlobalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x

# sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)

# print(context_emb.shape)
# print(sample_gsa(context_emb).shape)

#The causal self attention layer
'''
A causal model is efficient in two ways:
 1. In training, it lets you compute loss for every location in the output sequence while executing the model just once.
 2. During inference, for each new token generated you only need to calculate its outputs, 
    the outputs for the previous sequence elements can be reused.

The causal mask ensures that each location only has access to the locations that come before it

'''

class CausalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x,
        use_causal_mask = True)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x

# sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)

# print(targ_emb.shape)
# print(sample_csa(targ_emb).shape)

class FeedForward(tf.keras.layers.Layer):
  def __init__(self, d_model, dff, dropout_rate=0.1):
    super().__init__()
    self.seq = tf.keras.Sequential([
      tf.keras.layers.Dense(dff, activation='relu'),
      tf.keras.layers.Dense(d_model),
      tf.keras.layers.Dropout(dropout_rate)
    ])
    self.add = tf.keras.layers.Add()
    self.layer_norm = tf.keras.layers.LayerNormalization()

  def call(self, x):
    x = self.add([x, self.seq(x)])
    x = self.layer_norm(x) 
    return x

# sample_ffn = FeedForward(512, 2048)

# print(targ_emb.shape)
# print(sample_ffn(targ_emb).shape)

class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
    super().__init__()

    self.self_attention = GlobalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x):
    x = self.self_attention(x)
    x = self.ffn(x)
    return x

# sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)

# print(context_emb.shape)
# print(sample_encoder_layer(context_emb).shape)

'''
The encoder consists of:

  A PositionalEmbedding layer at the input.
  A stack of EncoderLayer layers.

'''
class Encoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):
    super().__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)

    self.enc_layers = [ EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate) 
                          for _ in range(num_layers)]
    self.dropout = tf.keras.layers.Dropout(dropout_rate)

  def call(self, x):
    # `x` is token-IDs shape: (batch, seq_len)
    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.

    # Add dropout.
    x = self.dropout(x)

    for i in range(self.num_layers):
      x = self.enc_layers[i](x)

    return x  # Shape `(batch_size, seq_len, d_model)`.

# Instantiate the encoder.
# sample_encoder = Encoder(num_layers=4,
#                          d_model=512,
#                          num_heads=8,
#                          dff=2048,
#                          vocab_size=8500)

# sample_encoder_output = sample_encoder(context, training=False)

# # Print the shape.
# print(context.shape)
# print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`.

#The decoder layer
class DecoderLayer(tf.keras.layers.Layer):
  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):
    
    super(DecoderLayer, self).__init__()

    self.causal_self_attention = CausalSelfAttention( num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)

    self.cross_attention = CrossAttention( num_heads=num_heads, key_dim=d_model,dropout=dropout_rate)

    self.ffn = FeedForward(d_model, dff)

  def call(self, x, context):
    x = self.causal_self_attention(x=x)
    x = self.cross_attention(x=x, context=context)

    # Cache the last attention scores for plotting later
    self.last_attn_scores = self.cross_attention.last_attn_scores

    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
    return x

# sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)

# sample_decoder_layer_output = sample_decoder_layer(x=targ_emb, context=context_emb)

# print(targ_emb.shape)
# print(context_emb.shape)
# print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`

class Decoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,
               dropout_rate=0.1):
    super(Decoder, self).__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,
                                             d_model=d_model)
    self.dropout = tf.keras.layers.Dropout(dropout_rate)
    self.dec_layers = [
        DecoderLayer(d_model=d_model, num_heads=num_heads,
                     dff=dff, dropout_rate=dropout_rate)
        for _ in range(num_layers)]

    self.last_attn_scores = None

  def call(self, x, context):
    # `x` is token-IDs shape (batch, target_seq_len)
    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)

    x = self.dropout(x)

    for i in range(self.num_layers):
      x  = self.dec_layers[i](x, context)

    self.last_attn_scores = self.dec_layers[-1].last_attn_scores

    # The shape of x is (batch_size, target_seq_len, d_model).
    return x

# Instantiate the decoder.
# sample_decoder = Decoder(num_layers=4,
#                          d_model=512,
#                          num_heads=8,
#                          dff=2048,
#                          vocab_size=8000)

# output = sample_decoder(
#     x=en,
#     context=pt_emb)

# # Print the shapes.
# print(en.shape)
# print(pt_emb.shape)
# print(output.shape)

#The Transformer
class Transformer(tf.keras.Model):
  def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
    super().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=input_vocab_size,
                           dropout_rate=dropout_rate)

    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=target_vocab_size,
                           dropout_rate=dropout_rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)

  def call(self, inputs):
    # To use a Keras model with `.fit` you must pass all your inputs in the
    # first argument.
    context, x  = inputs

    context = self.encoder(context)  # (batch_size, context_len, )

    x = self.decoder(x, context)  # (batch_size, target_len, d_model)

    # Final linear layer output.
    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits

num_layers = 2
d_model = 128
dff = 512
num_heads = 4
dropout_rate = 0.1

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=tokenizers.en.get_vocab_size().numpy(),
    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),
    dropout_rate=dropout_rate)

# output = transformer((pt, en))

# print(en.shape)
# print(pt.shape)
# print(output.shape)

# attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores
# print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)

# transformer.summary()

#Training
'''
  Set up the optimizer
   Adam optimizer with a custom learning rate scheduler according to the formula in the original Transformer paper.

'''
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super().__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    step = tf.cast(step, dtype=tf.float32)
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)

#Loss and Acc
def masked_loss(label, pred):
  mask = label != 0
  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=True, reduction='none')
  loss = loss_object(label, pred)

  mask = tf.cast(mask, dtype=loss.dtype)
  loss *= mask

  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
  return loss


def masked_accuracy(label, pred):
  pred = tf.argmax(pred, axis=2)
  label = tf.cast(label, pred.dtype)
  match = label == pred

  mask = label != 0

  match = match & mask

  match = tf.cast(match, dtype=tf.float32)
  mask = tf.cast(mask, dtype=tf.float32)
  return tf.reduce_sum(match)/tf.reduce_sum(mask)

transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

checkpoint_path =   f"{DATA_LIMIT}_cp.ckpt"
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, verbose=1)

transformer.fit(train_batches.repeat(),
                epochs=10,
                steps_per_epoch = 20,
                validation_data=val_batches, 
                callbacks=[])

'''
RUN INFERENECE

  1.Encode the input sentence using the Portuguese tokenizer (tokenizers.pt). This is the encoder input.
  2.The decoder input is initialized to the [START] token.
  3.Calculate the padding masks and the look ahead masks.
  4.The decoder then outputs the predictions by looking at the encoder output and its own output (self-attention).
  5.Concatenate the predicted token to the decoder input and pass it to the decoder.
  6.In this approach, the decoder predicts the next token based on the previous tokens it predicted.

'''
class Translator(tf.Module):
  def __init__(self, tokenizers, transformer):
    self.tokenizers = tokenizers
    self.transformer = transformer

  def __call__(self, sentence, max_length=MAX_TOKENS):
    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.
    assert isinstance(sentence, tf.Tensor)
    if len(sentence.shape) == 0:
      sentence = sentence[tf.newaxis]

    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()

    encoder_input = sentence

    # As the output language is English, initialize the output with the
    # English `[START]` token.
    start_end = self.tokenizers.en.tokenize([''])[0]
    start = start_end[0][tf.newaxis]
    end = start_end[1][tf.newaxis]

    # `tf.TensorArray` is required here (instead of a Python list), so that the
    # dynamic-loop can be traced by `tf.function`.
    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break

    output = tf.transpose(output_array.stack())
    # The output shape is `(1, tokens)`.
    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.

    tokens = tokenizers.en.lookup(output)[0]

    # `tf.function` prevents us from using the attention_weights that were
    # calculated on the last iteration of the loop.
    # So, recalculate them outside the loop.
    self.transformer([encoder_input, output[:,:-1]], training=False)
    attention_weights = self.transformer.decoder.last_attn_scores

    return text, tokens, attention_weights

translator = Translator(tokenizers, transformer)

sentence = 'Fuck'
translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))
print(translated_text)

transformer.save_weights("transformer_weights_v1_colab.tf")

class ExportTranslator(tf.Module):
  def __init__(self, translator):
    self.translator = translator

  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
  def __call__(self, sentence):
    (result,
     tokens,
     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)

    return result

translator = ExportTranslator(translator)
tf.saved_model.save(translator, export_dir='translator')

# !zip -r /content/translator.zip /content/translator

